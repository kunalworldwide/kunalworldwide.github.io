[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nComprehensive Guide: Running GPU Workloads on Kubernetes\n\n\n\n\n\n\nKubernetes\n\n\nGPU\n\n\nCloud Computing\n\n\nDevOps\n\n\n\nDeep dive into running and managing GPU workloads on Kubernetes clusters, including setup, optimization, and best practices\n\n\n\n\n\nDec 6, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nSetting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps\n\n\n\n\n\n\nPython\n\n\nDevOps\n\n\nCode Quality\n\n\nCI/CD\n\n\n\nA complete guide to implementing code quality checks, testing, and validation for Python projects using Azure DevOps\n\n\n\n\n\nOct 3, 2024\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Azure Storage Services üåêüóÑÔ∏è\n\n\n\n\n\n\nAzure\n\n\nStorage\n\n\nCloud Infrastructure\n\n\n\nA comprehensive guide to understanding different Azure storage solutions including Blob, File, Queue, Table, and Disk storage\n\n\n\n\n\nJan 19, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nSeamless Integration and Deployment of Azure Data Factory using Azure DevOps\n\n\n\n\n\n\nAzure\n\n\nDevOps\n\n\nData Factory\n\n\nCI/CD\n\n\n\nLearn how to implement CI/CD for Azure Data Factory using Azure DevOps, including source control integration and automated deployments\n\n\n\n\n\nNov 3, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Kubernetes: Essential Cluster Management Tools\n\n\n\n\n\n\nKubernetes\n\n\nDevOps\n\n\nCloud Native\n\n\nTools\n\n\n\nA deep dive into the must-know cluster management tools for Kubernetes DevOps engineers\n\n\n\n\n\nAug 25, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nMastering Private Repositories in Enterprise with GitHub\n\n\n\n\n\n\nGitHub\n\n\nVersion Control\n\n\nEnterprise\n\n\nDevOps\n\n\n\nA comprehensive guide to effectively managing private repositories in enterprise environments using GitHub\n\n\n\n\n\nJul 19, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Azure Web App CI/CD with Terraform and Azure DevOps\n\n\n\n\n\n\nAzure\n\n\nDevOps\n\n\nTerraform\n\n\nWeb Apps\n\n\nCI/CD\n\n\n\nStep-by-step guide to setting up a complete CI/CD pipeline for Azure Web Apps using Terraform and Azure DevOps\n\n\n\n\n\nJul 7, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCost Estimation for Infrastructure: A Complete Guide\n\n\n\n\n\n\nInfrastructure\n\n\nCost Management\n\n\nDevOps\n\n\nCloud\n\n\n\nLearn how to effectively estimate and manage infrastructure costs using tools like Infracost\n\n\n\n\n\nMar 7, 2023\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nStreamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps\n\n\n\n\n\n\nAzure\n\n\nDevOps\n\n\nSynapse\n\n\nCI/CD\n\n\n\nA comprehensive guide to implementing CI/CD for Azure Synapse Analytics and SQL pools using Azure DevOps\n\n\n\n\n\nFeb 23, 2023\n\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to publish from Release Pipeline in Azure DevOps\n\n\n\n\n\n\nAzure DevOps\n\n\nCI/CD\n\n\nGit\n\n\nDevOps\n\n\n\nA step-by-step guide to publishing artifacts from Azure DevOps release pipelines using git repositories\n\n\n\n\n\nDec 26, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive Azure Best Practices Guide\n\n\n\n\n\n\nAzure\n\n\nSecurity\n\n\nBest Practices\n\n\nCloud\n\n\n\nA complete guide to Azure best practices covering security, management, operations, and service-specific recommendations\n\n\n\n\n\nDec 3, 2022\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe trick to not using a self-hosted agent in Azure DevOps\n\n\n\n\n\n\nAzure\n\n\nDevOps\n\n\nSecurity\n\n\nInfrastructure\n\n\n\nLearn how to overcome firewall restrictions in Azure DevOps pipelines without using self-hosted agents\n\n\n\n\n\nNov 18, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform\n\n\n\n\n\n\nAzure\n\n\nTerraform\n\n\nDevOps\n\n\nData Engineering\n\n\n\nA comprehensive guide to deploying Azure data services using Terraform\n\n\n\n\n\nOct 12, 2022\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks & Presentations",
    "section": "",
    "text": "I regularly speak at tech conferences and meetups about Cloud Computing, DevOps, and Data Engineering. Here are some of my recent talks and presentations.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nPolicy Based Control for Cloud Native Environment with OPA\n\n\n\nKubernetes\n\n\nSecurity\n\n\nOPA\n\n\nCloud Native\n\n\n\nExploring policy-based control using Open Policy Agent and Gatekeeper in Kubernetes environments\n\n\n\nAugust 24, 2024\n\n\n\n\n\nlocation\n\n\nBengaluru, India\n\n\n\n\nvenue\n\n\nDocker DevTools Day 3.0 at Wissen Infotech\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to KEDA\n\n\n\nKubernetes\n\n\nKEDA\n\n\nAutoscaling\n\n\nCloud Native\n\n\n\nA comprehensive overview of Kubernetes Event-Driven Autoscaling, covering architecture, implementation, and real-world use cases\n\n\n\nMay 18, 2024\n\n\n\n\n\nlocation\n\n\nBengaluru, India\n\n\n\n\nvenue\n\n\nKubetools Day 2.0 at Guidewire Software\n\n\n\n\n\n\n\n\n\n\n\n\nContainerization at Scale: Kubernetes Deep Dive\n\n\n\nKubernetes\n\n\nContainers\n\n\nScaling\n\n\nCost Optimization\n\n\n\nAdvanced strategies for scaling Kubernetes beyond basic autoscaling with cost optimization\n\n\n\nMay 11, 2024\n\n\n\n\n\nlocation\n\n\nBengaluru, India\n\n\n\n\nvenue\n\n\nRed Hat India Pvt. Ltd, Kubernetes Bangalore May Meetup\n\n\n\n\n\n\n\n\n\n\n\n\nContainerization at Scale: Challenges and Solutions\n\n\n\nContainers\n\n\nMicroservices\n\n\nSRE\n\n\nScaling\n\n\n\nDeep dive into scaling microservices efficiently with cost optimization strategies\n\n\n\nMay 4, 2024\n\n\n\n\n\nlocation\n\n\nBengaluru, India\n\n\n\n\nvenue\n\n\nTata 1mg Tech center, Union St, Bangalore SRE May Meetup\n\n\n\n\n\n\n\n\n\n\n\n\nImportance of DevOps in Modern Software Development Lifecycle\n\n\n\nDevOps\n\n\nSoftware Development\n\n\nBest Practices\n\n\n\nUnderstanding the critical role of DevOps in contemporary software development with practical demonstrations\n\n\n\nDecember 22, 2023\n\n\n\n\n\nlocation\n\n\nKolkata, India\n\n\n\n\nvenue\n\n\nGoogle DevFest Kolkata 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTelecom Churn Prediction\n\n\n\nMachine Learning\n\n\nTelecom\n\n\nAnalytics\n\n\n\nAnalyzing customer-level data to predict and reduce churn in high-value telecom customers\n\n\n\nJuly 11, 2023\n\n\n\n\n\nlocation\n\n\nLondon, UK\n\n\n\n\nvenue\n\n\nInternational Institute of Information Technology\n\n\n\n\n\n\n\n\n\n\n\n\nLead Scoring Model for Education Institute\n\n\n\nMachine Learning\n\n\nEducation\n\n\nCase Study\n\n\n\nA comprehensive approach to developing a lead scoring model achieving 80% conversion rate for educational institutions\n\n\n\nMay 23, 2023\n\n\n\n\n\nlocation\n\n\nBangalore, India\n\n\n\n\nvenue\n\n\nInternational Institute of Information Technology\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "",
    "text": "Azure, Microsoft‚Äôs cloud computing service, offers a range of storage solutions designed to meet the diverse needs of modern businesses. In this guide, we‚Äôll delve into the various Azure storage services, exploring their unique features and use cases. Whether you‚Äôre a developer, a data scientist, or an IT professional, understanding these services can enhance your cloud strategy."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#table-of-contents",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#table-of-contents",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nExploring Azure Storage Services üåêüóÑÔ∏è\n\n\nTable of Contents\nIntroduction to Azure Storage\nAzure Blob Storage\n\nWhat is Blob Storage? ü§î\nKey Features üåü\nUse Cases üõ†Ô∏è\n\nAzure File Storage\n\nWhat is File Storage? üìÇ\nKey Features üåü\nUse Cases üõ†Ô∏è\n\nAzure Queue Storage\n\nWhat is Queue Storage? üì®\nKey Features üåü\nUse Cases üõ†Ô∏è\n\nAzure Table Storage\n\nWhat is Table Storage? üìä\nKey Features üåü\nUse Cases üõ†Ô∏è\n\nAzure Disk Storage\n\nWhat is Disk Storage? üíΩ\nKey Features üåü\nUse Cases üõ†Ô∏è\n\nChoosing the Right Azure Storage Service\nConclusion\nRead my blogs:\nConnect with Me:"
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#introduction-to-azure-storage",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#introduction-to-azure-storage",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Introduction to Azure Storage",
    "text": "Introduction to Azure Storage\nAzure Storage is a cloud service at the core of Azure. It offers scalable, durable, and secure storage options for a variety of data types. In this guide, we will explore the different Azure Storage services, understand their functionalities, and see how they can be applied in real-world scenarios."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#azure-blob-storage",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#azure-blob-storage",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Azure Blob Storage",
    "text": "Azure Blob Storage\n\nWhat is Blob Storage? ü§î\nBlob Storage is Azure‚Äôs object storage solution for the cloud. It is optimized for storing massive amounts of unstructured data, such as text or binary data.\n\n\nKey Features üåü\n\nScalability: Easily handles massive amounts of data.\nSecurity: Advanced security and encryption features.\nAccessibility: Data is accessible from anywhere in the world over HTTP or HTTPS.\n\n\n\nUse Cases üõ†Ô∏è\n\nStoring Images and Videos: Ideal for storing media files for websites and applications.\nData Backup: Efficient for backing up data and disaster recovery solutions.\nBig Data Analysis: Can store large datasets for analytics purposes."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#azure-file-storage",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#azure-file-storage",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Azure File Storage",
    "text": "Azure File Storage\n\nWhat is File Storage? üìÇ\nAzure File Storage offers fully managed file shares in the cloud using the standard SMB protocol.\n\n\nKey Features üåü\n\nSMB Protocol: Uses the standard SMB 3.0 protocol.\nShared Access: Files can be shared across applications and virtual machines.\n\n\n\nUse Cases üõ†Ô∏è\n\nLift and Shift: Ideal for migrating on-premises applications that rely on file shares to Azure.\nHybrid Storage: Works well for scenarios requiring both on-premises and cloud storage."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#azure-queue-storage",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#azure-queue-storage",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Azure Queue Storage",
    "text": "Azure Queue Storage\n\nWhat is Queue Storage? üì®\nQueue Storage provides a messaging queue for reliable messaging between application components.\n\n\nKey Features üåü\n\nDecoupling: Helps in decoupling application components.\nScalability: Scales to handle a large number of messages.\n\n\n\nUse Cases üõ†Ô∏è\n\nMessage Processing: Perfect for asynchronous message processing in applications.\nTask Scheduling: Useful in task scheduling scenarios."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#azure-table-storage",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#azure-table-storage",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Azure Table Storage",
    "text": "Azure Table Storage\n\nWhat is Table Storage? üìä\nTable Storage is a NoSQL data store for semi-structured data.\n\n\nKey Features üåü\n\nNoSQL: Ideal for storing large volumes of non-relational data.\nFlexible: Easy to adapt to changing data requirements.\n\n\n\nUse Cases üõ†Ô∏è\n\nStoring User Data: Great for storing user data for web applications.\nAddress Books: Suitable for storing address books, user profiles, etc."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#azure-disk-storage",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#azure-disk-storage",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Azure Disk Storage",
    "text": "Azure Disk Storage\n\nWhat is Disk Storage? üíΩ\nAzure Disk Storage offers high-performance, durable block storage for Azure Virtual Machines.\n\n\nKey Features üåü\n\nHigh Performance: Designed for I/O-intensive workloads.\nData Durability: Provides persistent data storage.\n\n\n\nUse Cases üõ†Ô∏è\n\nVirtual Machines: Ideal for databases and other high-performance applications running on VMs."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#choosing-the-right-azure-storage-service",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#choosing-the-right-azure-storage-service",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Choosing the Right Azure Storage Service",
    "text": "Choosing the Right Azure Storage Service\nWhen selecting an Azure Storage service, consider factors like data type, access patterns, and scalability requirements. Each service is tailored to specific scenarios, so understanding your application‚Äôs needs is crucial."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#conclusion",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#conclusion",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Conclusion",
    "text": "Conclusion\nAzure‚Äôs diverse storage options offer flexible, scalable, and secure solutions to meet the ever-evolving data storage needs of businesses. By understanding the strengths and applications of each service, you can make informed decisions about your cloud storage strategy."
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#read-my-blogs",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#read-my-blogs",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2024-01-19-exploring-azure-storage-services.html#connect-with-me",
    "href": "posts/2024-01-19-exploring-azure-storage-services.html#connect-with-me",
    "title": "Exploring Azure Storage Services üåêüóÑÔ∏è",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html",
    "href": "posts/2022-12-03-azure-best-practices.html",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "",
    "text": "Kunal Das, Author\nAzure best practices assist businesses in making the most of Azure resources to create and maintain scalable, cost-efficient, and secure solutions on the Microsoft Azure cloud. Having the correct Azure best practices can make or ruin a firm because Azure serves as the foundation of many modern organizations. I‚Äôve covered the fundamental best practices that every Azure administrator needs to be aware of in this document. I‚Äôve also provided advice on how to create a secure, reliable, and effective Azure infrastructure. These best practices must be followed in tandem because none of them by themselves can adequately secure the systems. You must select the best security alternatives based on your surroundings and demands, as is always the case."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#security",
    "href": "posts/2022-12-03-azure-best-practices.html#security",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Security:",
    "text": "Security:\nthe most important thing before anything else is security, The recommendations below can help assure more robust Azure security, but they cannot serve as a complete substitute. The top practices that I believe will help you strengthen and safeguard your Azure are listed below."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#enable-azure-ad-conditional-access-and-enforce-mfa",
    "href": "posts/2022-12-03-azure-best-practices.html#enable-azure-ad-conditional-access-and-enforce-mfa",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Enable Azure AD Conditional Access and enforce MFA",
    "text": "Enable Azure AD Conditional Access and enforce MFA\nAzure AD Conditional Access assists administrators in enabling users to be productive whenever and wherever they choose while simultaneously safeguarding the assets of the company. Automating access control based on security, business, and compliance requirements are made possible via conditional access. Access to data and applications is protected by the addition of a crucial security layer provided by Azure AD Multi-Factor Authentication. To design an Azure Active Directory conditional access deployment and evaluate deployment considerations for Azure AD Multi-Factor Authentication, go to these Azure documentation sites (MFA)."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#disable-rdpssh-from-the-internet",
    "href": "posts/2022-12-03-azure-best-practices.html#disable-rdpssh-from-the-internet",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Disable RDP/SSH from the Internet",
    "text": "Disable RDP/SSH from the Internet\nUse JIT and Azure Bastion Do not expose RDP and SSH access over the Internet in order to provide remote access to Windows and Linux VMs in Azure from anywhere in the world without compromising security. To provide safe remote access to Azure virtual machines, use one of these techniques:\n‚úì Enable site-to-site VPN or ExpressRoute connections for Just-In-Time (JIT) VM access:\nJIT offers time-limited VM access through RDP and SSH, which lessens the exposure to brute force assaults. In essence, Network Security Groups (NSGs) lock off RDP and SSH ports and only permit authorized users to access them for a predetermined time. Users can request access to JIT VMs using Azure AD and role-based Access Control (RBAC) permissions.\n‚úì Configure Azure Bastion inside your virtual network.\nDirect RDP/SSH communication to your VMs over TLS from the Azure interface is made possible by Azure Bastion. The PaaS service Azure Bastion does away with the requirement for VMs to have public IP addresses, agents, or specialized client software.\n\nSecure privileged access with Azure AD PIM\nAzure AD Privileged Identity Management enables management, control, and monitoring of access to vital resources in Azure, Microsoft 365, and Intune (PIM). Admins must activate or elevate their privilege through PIM in order to use it for a brief period of time."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#management",
    "href": "posts/2022-12-03-azure-best-practices.html#management",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Management:",
    "text": "Management:\nUse Resource Groups; Tag individual resources:\nCreate a resource group strategy that meets your company‚Äôs needs, then plan, create, and implement it. Resource groups, which are logical collections of Azure resources, containerize related resources in a group for administration simplicity, security, and cost tracking for your workloads.\n\nA best practice resource group strategy should include the following: Group resources by\nEnvironment: prod, dev, uat,stg,perf,sit\nApplication: BI,DWH\nBusiness Unit: ML,DS\nFollow a well-defined naming standard\nInclude resource type, application or business unit, environment, Azure region, and consecutive entity number etc.\nFor example, dev-ause-asy-01 means Azure Synapse Workspace in Australia South East Region which is in DEV environment.\nLeverage Resource tagging\nTag all resources inside resource groups to communicate valuable information to your teams, discover resources, and manage costs and it is easy to delete resources as well."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#operations",
    "href": "posts/2022-12-03-azure-best-practices.html#operations",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Operations:",
    "text": "Operations:\nUse Azure Advisor:\nAzure Advisor offers individualised, practical advice for cost, security, dependability, operational excellence, and performance. Organizations can optimise their deployments with Azure Advisor in accordance with Microsoft best practises. These suggestions are based on Microsoft best practises that are successful for most businesses and were compiled using resource configuration analysis and usage telemetry in your Azure tenant."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#cost-optimization",
    "href": "posts/2022-12-03-azure-best-practices.html#cost-optimization",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Cost Optimization:",
    "text": "Cost Optimization:\nUse reserved instances:\nReserved instances are useful in certain circumstances, such as when you frequently employ the same VM size across several VMs. Domain controllers operating on Azure are an excellent example. On these VMs, three-year reserved instances offer savings of up to 72%.\nDelete unneeded resources:\nAfter VMs are decommissioned, orphan resources are frequently forgotten about and left in a tenant. These resources are pricey! Common examples of these Azure orphan resources include network cards and OS discs. Fortunately, the Azure Portal assists in reminding administrators to delete unnecessary resources at provisioning and deletion time."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#services",
    "href": "posts/2022-12-03-azure-best-practices.html#services",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Services:",
    "text": "Services:\nIn this section let us see in detail the most used resources and the best way to use those,\n‚û¢ Virtual Machine\n‚úì Enforce multi-factor authentication (MFA) and complex passwords. MFA can help limit the threat of compromised credentials. Complex passwords help reduce the effectiveness of brute-force password attacks. ‚úì Use just-in-time (JIT) virtual machine access. JIT access works with NSGs and the Azure firewall and helps you layer in role-based access controls (RBAC) and time-bindings on access to virtual machines.\n‚úì Have a patch process in place. If you‚Äôre not patching your workloads, all your other efforts may be for nothing. A single unpatched vulnerability can lead to a breach. A patch process to keep your operating systems and applications up to date helps you mitigate this risk.\n‚úì Lock down administrative ports. Unless necessary, restrict access to SSH, RDP, WinRM, and other administrative ports.\n‚úì Use the Azure firewall and network security groups (NGSs) to limit access to workloads. Consistent with the principle of least privilege, use NSGs and the Azure firewall to restrict workload access.\n‚úì Apply the Latest OS Patches Ensure that the latest OS patches available for Microsoft Azure virtual machines are applied.\n‚úì Approved Azure Machine Image in Use Ensure that all your Azure virtual machine instances are launched from approved machine images only.\n‚úì Enable Auto-Shutdown Configure your Microsoft Azure virtual machines to automatically shut down on a daily basis.\n‚û¢ Storage\n‚úì Restrict database and storage access. UseFirewalls and access controls to limit what level of access users, devices, and services have to your databases and storage blobs.\n‚úì Leverage auditing. Turn on auditing for your Azure databases. Doing so enables you to gain visibility into all database changes.\n‚úì Configure threat detection for Azure SQL. If you use Azure SQL, activating threat detection helps you identify security issues faster and limit dwell time.\n‚úì Set log alerts in Azure Monitor. It isn‚Äôt enough to simply log events. Make sure you are alerting against security-related events in Azure Monitor so you can remediate issues quickly (and automatically when possible).\n‚úì Enable Azure Defender for your storage accounts. Azure Defender provides you with hardening and securing your Azure storage accounts.\n‚úì Use soft deletes. Soft deletes help you ensure data is still retrievable (for 14 days) in the event a malicious actor (or user error) leads to data you wanted to keep ‚Äî getting deleted.\n‚úì Use shared access signatures (SAS). SAS enables you to implement granular access controls and time limits on client access to data.\n‚úì Disable Anonymous Access to Blob Containers Ensure that anonymous access to blob containers is disabled within your Azure Storage account.\n‚úì Disable public access to storage accounts with blob containers Ensure that public access to blob containers is disabled for your Azure storage accounts to override any ACL configurations allowing access.\n‚û¢ Network\n‚úì Encrypt data in transit. As we mentioned in the encryption and data security section: encryption of data in transit (and at rest) is a must. Leverage modern encryption protocols for all network traffic.\n‚úì Implement zero trust. By default, network policies should deny access unless there is an explicit allow rule.\n‚úì Limit open ports and Internet-facing endpoints. Unless there is a well-defined business reason for a port to be open or workload to be Internet-facing, don‚Äôt let it happen.\n‚úì Monitor device access. Monitoring access to your workloads and devices (e.g.¬†using a SIEM or Azure Monitor) helps you proactively detect threats\n‚úì Segment your networks. Logical network segmentation can help improve visibility, make your networks easier to manage and limit east-west movement in the event of a breach.\n‚úì Check for NSG Flow Log Retention Period Ensure that the Network Security Group (NSG) flow log retention period is greater than or equal to 90 days.\n‚úì Check for Network Security Groups with Port Ranges Ensure there are no network security groups with a range of ports opened to allow incoming traffic.\n‚úì Enable DDoS Standard Protection for Virtual Networks Ensure that DDoS standard protection is enabled for production Azure virtual networks.\n‚úì Monitor Network Security Group Configuration Changes Network security group changes have been detected in your Microsoft Azure cloud account.\n‚úì Review Network Interfaces with IP Forwarding Enabled Ensure that the Azure network interfaces with IP forwarding enabled are regularly reviewed."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#database",
    "href": "posts/2022-12-03-azure-best-practices.html#database",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "‚û¢ Database",
    "text": "‚û¢ Database\n‚û¢ Cosmos DB:\n‚úì Enable Advanced Threat Protection Ensure that Advanced Threat Protection is enabled for all Microsoft Azure Cosmos DB accounts.\n‚úì Enable Automatic Failover Enable automatic failover for Microsoft Azure Cosmos DB accounts.\n‚úì Restrict Default Network Access for Azure Cosmos DB Accounts Ensure that default network access (i.e.¬†public access) is denied within your Azure Cosmos DB accounts configuration.\n‚û¢ SQL:\n‚úì Advanced Data Security for SQL Servers Ensure that Advanced Data Security (ADS) is enabled at the Azure SQL database server level.\n‚úì Check for Publicly Accessible SQL Servers Ensure that Azure SQL database servers are accessible via private endpoints only.\n‚úì Check for Sufficient Point in Time Restore (PITR) Backup Retention Period Ensure there is a sufficient PITR backup retention period configured for Azure SQL databases.\n‚úì Check for Unrestricted SQL Database Access Ensure that no SQL databases allow unrestricted inbound access from 0.0.0.0/0 (any IP address).\n‚úì Configure ‚ÄúAuditActionGroup‚Äù for SQL Server Auditing Ensure that ‚ÄúAuditActionGroup‚Äù property is well configured at the Azure SQL database server level.\n‚úì Configure Emails for Vulnerability Assessment Scan Reports and Alerts Ensure that ‚ÄúSend scan reports to‚Äù setting is configured for SQL database servers.\n‚úì Detect Create, Update, and Delete SQL Server Firewall Rule Events SQL Server firewall rule changes have been detected in your Microsoft Azure cloud account.\n‚úì Enable All Types of Threat Detection on SQL Servers Enable all types of threat detection for your Microsoft Azure SQL database servers.\n‚úì Enable Auditing for SQL Servers Ensure that database auditing is enabled at the Azure SQL database server level.\n‚úì Enable Auto-Failover Groups Ensure that your Azure SQL database servers are configured to use auto-failover groups.\n‚úì Enable Automatic Tuning for SQL Database Servers Ensure that Automatic Tuning feature is enabled for Microsoft Azure SQL database servers.\n‚úì Enable Transparent Data Encryption for SQL Databases Ensure that Transparent Data Encryption (TDE) is enabled for every Azure SQL database.\n‚úì Enable Vulnerability Assessment Email Notifications for Admins and Subscription Owners Ensure that the Vulnerability Assessment setting ‚ÄúAlso send email notification to admins and subscription owners‚Äù is enabled. Enable Vulnerability Assessment Periodic Recurring Scans Ensure that the Vulnerability Assessment Periodic Recurring Scans setting is enabled for SQL database servers.\n‚úì Enable Vulnerability Assessment for Microsoft SQL Servers Ensure that Vulnerability Assessment is enabled for Microsoft SQL database servers.\n‚úì SQL Auditing Retention Ensure that SQL database auditing has a sufficient log data retention period configured.\n‚úì Use Azure Active Directory Admin for SQL Authentication Ensure that an Azure Active Directory (AAD) admin is configured for SQL authentication.\n‚úì Use BYOK for Transparent Data Encryption Use Bring Your Own Key (BYOK) support for Transparent Data Encryption (TDE).\n‚û¢ PostgreSQL\n‚úì Check for PostgreSQL Log Retention Period Ensure that PostgreSQL database servers have a sufficient log retention period configured.\n‚úì Check for PostgreSQL Major Version Ensure that PostgreSQL database servers are using the latest major version of PostgreSQL database.\n‚úì Enable ‚ÄúCONNECTION_THROTTLING‚Äù Parameter for PostgreSQL Servers Ensure that ‚Äúconnection_throttling‚Äù parameter is set to ‚ÄúON‚Äù within your Azure PostgreSQL server settings.\n‚úì Enable ‚ÄúLOG_CHECKPOINTS‚Äù Parameter for PostgreSQL Servers Enable ‚Äúlog_checkpoints‚Äù parameter for your Microsoft Azure PostgreSQL database servers.\n‚úì Enable ‚ÄúLOG_CONNECTIONS‚Äù Parameter for PostgreSQL Servers Enable ‚Äúlog_connections‚Äù parameter for your Microsoft Azure PostgreSQL database servers.\n‚úì Enable ‚ÄúLOG_DISCONNECTIONS‚Äù Parameter for PostgreSQL Servers Enable ‚Äúlog_disconnections‚Äù parameter for your Microsoft Azure PostgreSQL database servers.\n‚úì Enable ‚ÄúLOG_DURATION‚Äù Parameter for PostgreSQL Servers Enable ‚Äúlog_duration‚Äù parameter on your Microsoft Azure PostgreSQL database servers.\n‚úì Enable ‚Äúlog_checkpoints‚Äù Parameter for PostgreSQL Flexible Servers Enable ‚Äúlog_checkpoints‚Äù parameter for your Microsoft Azure PostgreSQL flexible database servers.\n‚úì Enable In-Transit Encryption for PostgreSQL Database Servers Ensure that in-transit encryption is enabled for your Azure PostgreSQL database servers.\n‚úì Enable Infrastructure Double Encryption for Single Servers Ensure that infrastructure double encryption is enabled for Single Server Azure PostgreSQL database servers.\n‚úì Use Azure Active Directory Admin for PostgreSQL Authentication Ensure that an Azure Active Directory (AAD) admin is configured for PostgreSQL authentication.\n‚û¢ MySQL:\n‚úì Configure TLS Version for MySQL Flexible Database Servers Ensure that the ‚Äòtls_version‚Äô parameter is set to a minimum of ‚ÄòTLSV1.2‚Äô for all MySQL flexible database servers.\n‚úì Enable In-Transit Encryption for MySQL Servers Ensure that in-transit encryption is enabled for your Azure MySQL database servers."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#appservice",
    "href": "posts/2022-12-03-azure-best-practices.html#appservice",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "‚û¢ AppService:",
    "text": "‚û¢ AppService:\n‚úì Check for Latest Version of .NET Framework Enable HTTP to HTTPS redirects for your Microsoft Azure App Service web applications.\n‚úì Check for Latest Version of Java Ensure that Azure App Service web applications are using the latest stable version of Java.\n‚úì Check for Latest Version of PHP Ensure that Azure App Service web applications are using the latest version of PHP.\n‚úì Check for Latest Version of Python Ensure that Azure App Service web applications are using the latest version of Python.\n‚úì Check for Sufficient Backup Retention Period Ensure there is a sufficient backup retention period configured for Azure App Services applications.\n‚úì Check for TLS Protocol Latest Version Ensure that Azure App Service web applications are using the latest version of TLS encryption.\n‚úì Disable Plain FTP Deployment Ensure that FTP access is disabled for your Azure App Services web applications.\n‚úì Disable Remote Debugging Disable Remote Debugging feature for your Microsoft Azure App Services web applications.\n‚úì Enable Always On Ensure that your Azure App Services web applications stay loaded all the time by enabling the Always On feature.\n‚úì Enable App Service Authentication Ensure that App Service Authentication is enabled within your Microsoft Azure cloud account.\n‚úì Enable Application Insights Ensure that Azure App Services applications are configured to use Application Insights feature.\n‚úì Enable Automated Backups Ensure that all your Azure App Services applications are using the Backup and Restore feature.\n‚úì Enable FTPS-Only Access Enable FTPS-only access for your Microsoft Azure App Services web applications.\n‚úì Enable HTTP/2 Ensure that Azure App Service web applications are using the latest stable version of HTTP.\n‚úì Enable HTTPS-Only Traffic Enable HTTP to HTTPS redirects for your Microsoft Azure App Service web applications.\n‚úì Enable Incoming Client Certificates Ensure that Azure App Service web applications are using incoming client certificates.\n‚úì Enable Registration with Azure Active Directory Ensure that registration with Azure Active Directory is enabled for Azure App Service applications.\n‚úì Use Key Vaults to Store App Service Application Secrets Ensure that Azure Key Vaults are used to store App Service application secrets.\n‚û¢ KeyVault\n‚úì App Tier Customer-Managed Key In Use Ensure that a Customer-Managed Key is created for your Azure cloud application tier.\n‚úì Check for Allowed Certificate Key Types Ensure that Azure Key Vault certificates are using the appropriate key type(s).\n‚úì Check for Azure Key Vault Keys Expiration Date Ensure that your Azure Key Vault encryption keys are renewed prior to their expiration date.\n‚úì Check for Azure Key Vault Secrets Expiration Date Ensure that your Azure Key Vault secrets are renewed prior to their expiration date.\n‚úì Check for Certificate Minimum Key Size Ensure that Azure Key Vault RSA certificates are using the appropriate key size.\n‚úì Check for Key Vault Full Administrator Permissions Ensure that no Azure user, group or application has full permissions to access and manage Key Vaults.\n‚úì Check for Sufficient Certificate Auto-Renewal Period Ensure there is a sufficient period configured for the SSL certificates auto-renewal.\n‚úì Database Tier Customer-Managed Key In Use Ensure that a Customer-Managed Key is created for your Microsoft Azure cloud database tier.\n‚úì Enable AuditEvent Logging for Azure Key Vaults Ensure that AuditEvent logging is enabled for your Microsoft Azure Key Vaults.\n‚úì Enable Certificate Transparency Ensure that certificate transparency is enabled for all your Azure Key Vault certificates.\n‚úì Enable Key Vault Recoverability Ensure that your Microsoft Azure Key Vault instances are recoverable.\n‚úì Enable SSL Certificate Auto-Renewal Ensure that Auto-Renewal feature is enabled for your Azure Key Vault SSL certificates.\n‚úì Enable Trusted Microsoft Services for Key Vault Access Allow trusted Microsoft services to access your Azure Key Vault resources (i.e.¬†encryption keys, secrets and certificates).\n‚úì Restrict Default Network Access for Azure Key Vaults Ensure that default network access (i.e.¬†public access) rule is set to ‚ÄúDeny‚Äù within your Azure Key Vaults configuration.\n‚úì Set Azure Secret Key Expiration Ensure that an expiration date is set for all your Microsoft Azure secret keys.\n‚úì Set Encryption Key Expiration Ensure that an expiration date is configured for all your Microsoft Azure encryption keys.\n‚úì Web Tier Customer-Managed Key In Use Ensure that a Customer-Managed Key is created for your Microsoft Azure cloud web tier"
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#conclusion",
    "href": "posts/2022-12-03-azure-best-practices.html#conclusion",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Conclusion:",
    "text": "Conclusion:\nThere are numerous Azure features and services that require ongoing maintenance in terms of security. There are countless ways to attack a system, and poorly protected systems are the ones that hackers most frequently target. By keeping a few simple things in mind, you can strengthen your network considerably. With some investment and your work, you can make your Azure secure and robust by using a variety of Azure services."
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#credits",
    "href": "posts/2022-12-03-azure-best-practices.html#credits",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Credits:",
    "text": "Credits:\nhttps://learn.microsoft.com/en-us/azure/security/fundamentals/best-practices-and-patterns"
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#read-my-blogs",
    "href": "posts/2022-12-03-azure-best-practices.html#read-my-blogs",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2022-12-03-azure-best-practices.html#connect-with-me",
    "href": "posts/2022-12-03-azure-best-practices.html#connect-with-me",
    "title": "Comprehensive Azure Best Practices Guide",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "",
    "text": "image\nHey there, Almost everyone who deploys python code wants to have it thoroughly checked and follow proper standards but sometimes setting up a code quality pipeline in your CI-CD process can be cumbersome due to various reasons.\nMainly, 1. You have to setup for each individual pipelines which can be an issue since many of us might have multiple repo multiple project and we need to have it standardize across projects to make sure it just works without tinkering in every other repo 2. There are plenty of tools and finding the right set of tools and also making sure a certain set of tools work together without giving any issues 3. Publishing a output also matters which will help the developers easily see what is lacking and they can start improving on the same.\nPython developers and DevOps enthusiasts! Today, we‚Äôre going to walk through setting up a robust build validation pipeline for your Python projects using Azure DevOps. This guide will help you ensure code quality, maintain consistency across multiple repositories, and streamline your development process - all without relying on external tools.\nI was tasked not long ago to find a FREE solution without any external tool which should have below features\nWhile I started developing the build validation pipeline ( Azure DevOps term basically means successful run of a pipeline to merge the pull request to a particular branch) I started looking for coding standards mainly for python I stumbled upon many articles and was in a bit of confusion which one to follow, but respecting the popularity and unanimous use of PEP-8 made the decision much easier.\nRef :https://peps.python.org/pep-0008/\nOk on top of a pep-8 validator I thought of adding isort which is another great tool to validate if your imports are correctly sorted and on top of this I added a few things extra like BLACK if we look at the black defination we see\nRef : https://pypi.org/project/black/\nSo from code quality part I ended up using three tools or validators 1. Black (code formatter) 2. isort (import sorter) 3. Flake8 (linter)\nI also added pytest and code coverage checks on this pipeline just to make sure developers are diligent to write unit test cases before they push the code to main branch\nThe steps here are for Azure Devops Pipeline but it can be easily replicated to githuba ctions or any other ci-cd tools with very minimal to minor tweaking\nSo let‚Äôs look at the pipeline that I have been telling till now"
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-by-step-setup",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-by-step-setup",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step-by-Step Setup",
    "text": "Step-by-Step Setup\n\nStep 1: Create the Azure Pipeline YAML File\nFirst, create a file named azure-pipelines.yml in the root of your repository. This file will define our pipeline.\ntrigger:\n  - none\n\nvariables:\n  - name: sourcePath\n    value: 'sourcecode'\n  - name: coverageThreshold\n    value: 80\n  - group: Tokens\n  - name: CodeDirectory\n    value: '$(System.DefaultWorkingDirectory)'\n\npool:\n  vmimage: 'ubuntu-latest'\n\njobs:\n- job: BuildTestAndAnalyze\n  displayName: 'üöÄ Build, Test, and Analyze'\n  steps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: '3.9'\n    displayName: 'üêç Use Python 3.9'\n\n  # More steps will be added here\nThis sets up the basic structure of our pipeline, specifying the Python version and defining some variables we‚Äôll use later.\nNow if you see ths issue we need this build validation pipeline in multiple repositories which is a bit tricky since we all know if we add this yaml to a particular repo then it will only work on that and not any other repo instead I wanted it to be a seperate but clone the repo for which it get's triggered in the pull request.\n\nThe solution? \nlook at he below step :)\n\n\nStep 2: Clone the PR Repository\nAdd this step to clone the PR repository:\n  - bash: |\n      echo \"üöÄ Initiating repository clone process...\"\n      REPO_URL=$(System.PullRequest.SourceRepositoryURI)\n      AUTH_URL=$(echo $REPO_URL | sed \"s|cyncly-engineering@|$(Code_Read_PAT)@|\")\n      FULL_BRANCH=$(System.PullRequest.SourceBranch)\n      BRANCH=$(echo $FULL_BRANCH | sed 's|^refs/heads/||')\n      CLONE_DIR=$(System.DefaultWorkingDirectory)/cloned_repo\n      \n      git clone --branch \"$BRANCH\" \"$AUTH_URL\" \"$CLONE_DIR\"\n      \n      if [ $? -eq 0 ]; then\n        echo \"‚úÖ Git clone successful\"\n        echo \"##vso[task.setvariable variable=CodeDirectory]$CLONE_DIR\"\n      else\n        echo \"‚ùå Git clone failed\"\n        exit 1\n      fi\n    displayName: 'üîÑ Clone PR Repository'\nThis step clones the PR repository, allowing us to work with the latest code.\nthis is a bit tricky spart and for other ci-cd tool we need to modify this the most\n\n\nStep 3: Install Dependencies\nNext, let‚Äôs install the project dependencies and our code quality tools:\n  - script: |\n      echo \"üîß Setting up Python environment...\"\n      python -m pip install --upgrade pip\n      if [ -f $(sourcePath)/requirements.txt ]; then\n        pip install -r $(sourcePath)/requirements.txt\n      fi\n    workingDirectory: $(CodeDirectory)\n    displayName: 'üì¶ Install Project Dependencies'\n\n  - script: |\n      echo \"üõ†Ô∏è Installing code quality and testing tools...\"\n      pip install black flake8 isort pytest pytest-cov httpx\n    displayName: 'üõ†Ô∏è Install Code Quality and Testing Tools'\n\n\nStep 4: Run Code Quality Checks\nNow, let‚Äôs add steps to run our code quality checks:\n  - script: |\n      echo \"üñåÔ∏è Running Black formatter check...\"\n      black --check --line-length 79 $(sourcePath) \n    displayName: 'üñåÔ∏è Run Black'\n    workingDirectory: $(CodeDirectory)\n\n  - script: |\n      echo \"üîÄ Running isort import sorter check...\"\n      isort --check-only $(sourcePath)\n    displayName: 'üîÄ Run isort'\n    workingDirectory: $(CodeDirectory)\n\n  - script: |\n      echo \"üîç Running Flake8 linter...\"\n      flake8 $(sourcePath)\n    displayName: 'üîç Run Flake8'\n    workingDirectory: $(CodeDirectory)\nThese steps will check your code formatting, import sorting, and linting.\n\n\nStep 5: Run Tests and Generate Coverage Report\nAdd this step to run tests and generate a coverage report:\n  - script: |\n      echo \"üß™ Running unit tests and generating coverage report...\"\n      pytest --cov=$(sourcePath) --cov-report=xml --cov-report=html ./tests\n    displayName: 'üß™ Run Tests with pytest'\n    workingDirectory: $(CodeDirectory)\n\n\nStep 6: Check Code Coverage Threshold\nLet‚Äôs ensure our code meets a minimum coverage threshold:\n  - script: |\n      echo \"üìä Checking code coverage threshold...\"\n      coverage_percentage=$(python -c \"import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); root = tree.getroot(); print(float(root.attrib['line-rate']) * 100)\")\n      echo \"Current code coverage: $coverage_percentage%\"\n      echo \"Coverage threshold: $COVERAGE_THRESHOLD%\"\n      if (( $(echo \"$coverage_percentage &lt; $COVERAGE_THRESHOLD\" | bc -l) )); then\n        echo \"‚ùå ##vso[task.logissue type=error]Code coverage ($coverage_percentage%) is below the threshold of $COVERAGE_THRESHOLD%\"\n        exit 1\n      else\n        echo \"‚úÖ Code coverage ($coverage_percentage%) meets or exceeds the threshold of $COVERAGE_THRESHOLD%\"\n      fi\n    displayName: 'üìä Check Code Coverage Threshold'\n    workingDirectory: $(CodeDirectory)\n    env:\n      COVERAGE_THRESHOLD: $(coverageThreshold)\n\n\nStep 7: Publish Coverage Report\nFinally, let‚Äôs publish our coverage report:\n  - task: PublishCodeCoverageResults@1\n    inputs:\n      codeCoverageTool: 'cobertura'\n      summaryFileLocation: '$(CodeDirectory)/coverage.xml'\n      reportDirectory: '$(CodeDirectory)/htmlcov'\n    displayName: 'üìà Publish Coverage Report'\n\n  - publish: $(CodeDirectory)/htmlcov\n    artifact: CodeCoverageReport\n    displayName: 'üì¶ Publish Coverage Report as Artifact'"
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#setting-up-local-development-environment",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#setting-up-local-development-environment",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Setting Up Local Development Environment",
    "text": "Setting Up Local Development Environment\nTo ensure developers can run these checks locally, add the following instructions to your project‚Äôs README:\n\nPrerequisites\n\nPython 3.9\npip (Python package manager)\n\n\n\nSetting Up Your Environment\n\nClone the repository:\ngit clone &lt;repository-url&gt;\ncd &lt;repository-name&gt;\nCreate a virtual environment:\npython -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\nInstall dependencies:\npip install -r sourcecode/requirements.txt\npip install black flake8 isort pytest pytest-cov httpx\n\n\n\nRunning Checks Locally\nTo ensure your code passes the pipeline checks, run these commands from the project root:\n\nBlack (code formatting):\nblack --check --line-length 79 sourcecode\nisort (import sorting):\nisort --check-only sourcecode\nFlake8 (linting):\nflake8 sourcecode\npytest (unit tests and coverage):\npytest --cov=sourcecode --cov-report=xml --cov-report=html ./tests"
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#setting-up-pre-commit-hooks",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#setting-up-pre-commit-hooks",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Setting Up Pre-commit Hooks",
    "text": "Setting Up Pre-commit Hooks\nTo catch issues before they even make it to the pipeline, set up pre-commit hooks:\n\nInstall pre-commit:\npip install pre-commit\nCreate a .pre-commit-config.yaml file in your project root:\nrepos:\n- repo: https://github.com/psf/black\n  rev: 22.3.0\n  hooks:\n  - id: black\n    args: [--line-length=79]\n- repo: https://github.com/PyCQA/isort\n  rev: 5.10.1\n  hooks:\n  - id: isort\n- repo: https://github.com/PyCQA/flake8\n  rev: 4.0.1\n  hooks:\n  - id: flake8\nInstall the hooks:\npre-commit install"
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#configuring-isort",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#configuring-isort",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Configuring isort",
    "text": "Configuring isort\nTo ensure consistent import sorting that aligns with Black‚Äôs formatting, create a file named .isort.cfg in the sourcecode/ directory:\n[settings]\nprofile = black\nmulti_line_output = 3\ninclude_trailing_comma = True\nforce_grid_wrap = 0\nuse_parentheses = True\nensure_newline_before_comments = True\nline_length = 79\nok Now how can we setup this pipeline to make sure any of our code get‚Äôs scanned properly before it goes to the main branch?\nFirst we need a PAT token to clone any repo across azure devops organisation so to get that first then set up the pipeline as a branch policy."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-1-create-a-personal-access-token-pat",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-1-create-a-personal-access-token-pat",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step 1: Create a Personal Access Token (PAT)",
    "text": "Step 1: Create a Personal Access Token (PAT)\nTo clone repositories across your Azure DevOps organization, you‚Äôll need a PAT with appropriate permissions. Here‚Äôs how to create one:\n\nSign in to your Azure DevOps organization (https://dev.azure.com/{your-organization}).\nIn the top right corner, click on your profile picture and select ‚ÄúPersonal access tokens‚Äù. ![alt text]../media/image.png)\nClick on ‚ÄúNew Token‚Äù.\nGive your token a name (e.g., ‚ÄúBuild Validation Pipeline‚Äù).\nSet the organization to your Azure DevOps organization.\nFor expiration, choose an appropriate timeframe (e.g., 1 year).\nUnder ‚ÄúScopes‚Äù, select ‚ÄúCustom defined‚Äù and then check the following permissions:\n\nCode (Read & Write)\nBuild (Read & Execute)\n\nClick ‚ÄúCreate‚Äù. \nCopy the generated token and store it securely. You won‚Äôt be able to see it again."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-2-add-the-pat-as-a-pipeline-variable",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-2-add-the-pat-as-a-pipeline-variable",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step 2: Add the PAT as a Pipeline Variable",
    "text": "Step 2: Add the PAT as a Pipeline Variable\nNow that you have a PAT, you need to add it as a variable in your pipeline:\n\nIn Azure DevOps, go to ‚ÄúPipelines‚Äù &gt; ‚ÄúLibrary‚Äù.\nClick on ‚Äú+ Variable group‚Äù.\nName the group (e.g., ‚ÄúBuildValidationTokens‚Äù).\nAdd a new variable:\n\nName: Code_Read_PAT\nValue: Paste your PAT here\n\nCheck the ‚ÄúKeep this value secret‚Äù box.\nClick ‚ÄúSave‚Äù."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-3-update-your-pipeline-yaml",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-3-update-your-pipeline-yaml",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step 3: Update Your Pipeline YAML",
    "text": "Step 3: Update Your Pipeline YAML\nUpdate your azure-pipelines.yml to use the variable group:\nvariables:\n  - group: BuildValidationTokens\n  # ... other variables ...\n\n# ... rest of your pipeline configuration ..."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-4-create-a-branch-policy",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-4-create-a-branch-policy",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step 4: Create a Branch Policy",
    "text": "Step 4: Create a Branch Policy\nTo ensure this pipeline runs on all pull requests to your main branch:\n\nGo to your Azure DevOps project.\nNavigate to ‚ÄúRepos‚Äù &gt; ‚ÄúBranches‚Äù.\nFind your main branch (often named main or master).\nClick the ellipsis (‚Ä¶) next to the branch name and select ‚ÄúBranch policies‚Äù.\nUnder ‚ÄúBuild Validation‚Äù, click ‚Äú+ Add build policy‚Äù.\nSelect your build validation pipeline. \nSet ‚ÄúTrigger‚Äù to ‚ÄúAutomatic‚Äù.\nSet ‚ÄúPolicy requirement‚Äù to ‚ÄúRequired‚Äù.\nSet ‚ÄúBuild expiration‚Äù as per your preference (e.g., ‚ÄúImmediately‚Äù).\nClick ‚ÄúSave‚Äù."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-5-configure-branch-protection",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-5-configure-branch-protection",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step 5: Configure Branch Protection",
    "text": "Step 5: Configure Branch Protection\nTo prevent direct pushes to the main branch:\n\nStill in the branch policies page for your main branch.\nUnder ‚ÄúRequire a minimum number of reviewers‚Äù, check the box.\nSet the minimum number of reviewers (e.g., 1 or 2).\nOptionally, check ‚ÄúAllow requestors to approve their own changes‚Äù based on your team‚Äôs preferences.\nClick ‚ÄúSave‚Äù."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-6-update-repository-settings",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-6-update-repository-settings",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step 6: Update Repository Settings",
    "text": "Step 6: Update Repository Settings\nEnsure your repository settings align with your new policy:\n\nGo to ‚ÄúProject settings‚Äù &gt; ‚ÄúRepositories‚Äù.\nSelect your repository.\nUnder ‚ÄúPolicies‚Äù:\n\nCheck ‚ÄúRequire a minimum number of reviewers‚Äù.\nSet ‚ÄúMinimum number of reviewers‚Äù (should match your branch policy).\nCheck ‚ÄúCheck for linked work items‚Äù.\nCheck ‚ÄúCheck for comment resolution‚Äù.\nOptionally, check other policies as per your team‚Äôs needs.\n\nClick ‚ÄúSave‚Äù."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#step-7-educate-your-team",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#step-7-educate-your-team",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Step 7: Educate Your Team",
    "text": "Step 7: Educate Your Team\nFinally, make sure your team understands the new process:\n\nCreate documentation explaining the new pipeline and its checks.\nHighlight the importance of running checks locally before creating a pull request.\nExplain how to interpret and act on the pipeline results.\nEncourage the use of pre-commit hooks for catching issues early.\n\nBy following these steps, you‚Äôve now set up a robust system where:\n\nAll code changes must go through a pull request.\nEach pull request automatically triggers your build validation pipeline.\nThe pipeline checks code formatting, linting, tests, and coverage.\nPull requests cannot be merged until the pipeline passes and required reviews are completed.\n\nNow in the repo where you have added the build validation just try to merge the code into main branch you will the build validation pipeline will be automatically added in queue and you can see the logs from the pipeline\n\n\n\nalt text\n\n\nAfter that if we go into the pipeline one step back there will be two important things \non top left there will be a tab called code coverage upon clicking that you will be able to view line by line coverage report of each of the files \nThis setup ensures that all code going into your main branch meets your quality standards. Remember to periodically review and update your pipeline as your project evolves and new best practices emerge."
  },
  {
    "objectID": "posts/2024-10-03-python-build-validation-pipeline.html#conclusion",
    "href": "posts/2024-10-03-python-build-validation-pipeline.html#conclusion",
    "title": "Setting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps",
    "section": "Conclusion",
    "text": "Conclusion\nBy following these steps, you‚Äôll have a robust build validation pipeline that ensures code quality across your Python projects. This setup:\n\nEnforces consistent code style\nCatches potential bugs early\nEncourages comprehensive testing\nProvides clear feedback to developers\n\nRemember, the key to success is continuous improvement. Don‚Äôt hesitate to iterate on this pipeline as you learn what works best for your team and projects. Happy coding!\nIt takes lot of time to write up and share with comminity if you think this can benifit you do share the word"
  },
  {
    "objectID": "posts/2022-11-18-azure-devops-agent-trick.html",
    "href": "posts/2022-11-18-azure-devops-agent-trick.html",
    "title": "The trick to not using a self-hosted agent in Azure DevOps",
    "section": "",
    "text": "Kunal Das, Author\nRead on :\nWell, when you want to design the build and release pipeline for any software development the one thing that comes to mind is where to do the whole operation.\napparently, There are two ways of doing it,\nThe free and easy way is to use a Microsoft Hosted build agent,\nBut in some cases, we may need to use a Self-hosted build agent,\nlet‚Äôs see what Microsoft has to say about self-hosted build agents.\nSo, Self-hosted build agents do provide some benefits, but in my experience, some of these benefits can be achieved in Microsoft-hosted build agents as well.\nFor instance, if you have a relatively small build/release pipeline there‚Äôs no need to cache dependencies as it may give you a faster build time but do you really need it?\nNow let‚Äôs see what a Microsoft hosted agent is as per them,\nSo as you can see these agents are ephemeral by nature so we can not use them for something permanent right?\nwell, yes but there are some other wat to look at it.\nIn this article, I have a solution for a different problem, which is using a self-hosted build agent to overcome firewall restrictions set in different azure resources.\nBut first, let‚Äôs understand the problem.\nFirewall restriction in a Synapse workspace\nIn the image, you can see public network access can be disabled. in that case, you will not be able to connect to this resource from the DevOps pipeline,\nIn some places, I have seen people using a self-hosted agent just for this purpose but actually, this is not required.\nThe process here is adding the IP address as a rule into the firewall\nand you can do just that from the CI/CD pipeline itself,\nLet‚Äôs look at how to achieve that.\nI am taking the example of Synapse,\nSo just before the deployment task add an Azure CLI task in the pipeline.\nand write the PowerShell script to add the build agent IP into the synapse workspace, needless to say, you need to give the service connection proper role to perform the action.\nIf you like to use Yaml then here is the code!\nNow, as we can add the firewall rule we can delete the rule too, for that use the below code\nand that is it.\nonce you run the deployment it will copy the IP to the build agent add it into the firewall rule then run the deployment and then delete it, but\nFeel free to reach me incase of any issues!!\nadios"
  },
  {
    "objectID": "posts/2022-11-18-azure-devops-agent-trick.html#read-my-blogs",
    "href": "posts/2022-11-18-azure-devops-agent-trick.html#read-my-blogs",
    "title": "The trick to not using a self-hosted agent in Azure DevOps",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2022-11-18-azure-devops-agent-trick.html#connect-with-me",
    "href": "posts/2022-11-18-azure-devops-agent-trick.html#connect-with-me",
    "title": "The trick to not using a self-hosted agent in Azure DevOps",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html",
    "href": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html",
    "title": "How to publish from Release Pipeline in Azure DevOps",
    "section": "",
    "text": "Kunal Das, Author\nIn this section, I shall describe how you can get the artifact from the release pipeline,\nPublishing any artifact from the build pipeline is pretty easy and there are plenty of tutorials available on the internet,\nTo publish an artifact from a build pipeline in Azure DevOps, you can use the Publish Build Artifacts task. Here‚Äôs how you can do it:\nThat‚Äôs it! The Publish Build Artifacts task will publish the specified artifacts to the pipeline or file share you specified. You can then use the artifact in a release pipeline or download it from the Artifacts page in Azure DevOps.\nNow for any reason, you may want to get the same result from the release pipeline as well, but unfortunately, the PublishPipelineArtifact@1 task does not support in release pipeline and even if you run it you will get some error, to solve this you can do one thing,"
  },
  {
    "objectID": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html#pushing-the-artifact-in-a-git-repo",
    "href": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html#pushing-the-artifact-in-a-git-repo",
    "title": "How to publish from Release Pipeline in Azure DevOps",
    "section": "Pushing the artifact in a git repo",
    "text": "Pushing the artifact in a git repo\nin this way essentially you can save the copy of your updated artifacts in a separate folder inside your git repo.\nlet me show my scenario,\nwe are following the git flow branching strategy, so as you can see in each environment we need to save a copy of the updated ARM template .\n\nHow did I achieve this?\nWell, let me guide you step by step.\nFirst, give all the permission required so that the pipeline can access the repo and push to it!\nGo to project settings ‚Üí Repository ‚Üí select your repo ‚Üísecurity ‚Üí project collection Administrators ‚Üí contribute ‚Üí ALLOW\n\nThis setting essentially allows pushing from the build pipeline to the repo.\nalso, you have to check one more setting,\nfor the classic pipeline, select the below option.\n\nand for YAML, add the below code before the YAML code.\nvariables:\n  system_accesstoken: $(System.AccessToken)\nnow add this bash task in the pipeline.\nsteps:\n- bash: |\n   git config --global user.email \"azuredevops@microsoft.com\"\n   git config --global user.name \"Azure DevOps\"\n   \n   REPO=\"$(System.TeamFoundationCollectionUri)$(System.TeamProject)/_git/$(Build.Repository.Name)\"\n   EXTRAHEADER=\"Authorization: Bearer $(System.AccessToken)\"\n   git -c http.extraheader=\"$EXTRAHEADER\" clone $REPO \n   cd $(Build.Repository.Name)\n   \n   mkdir 'QA-env'\n   cd 'QA-env'\n   \n   cp '$(System.DefaultWorkingDirectory)/ARM/TemplateForWorkspace.json' .\n   cp '$(System.DefaultWorkingDirectory)/ARM/TemplateParametersForWorkspace.json' .\n   \n   cd ..\n   \n   git add Template-QA/TemplateForWorkspace.json\n   git add Template-QA/TemplateParametersForWorkspace.json\n   \n   \n   MAINBRANCHNAME=main\n   git config http.$REPO.extraHeader \"$EXTRAHEADER\"\n   git commit -a -m \"added QA json updated files\"\n   \n   echo -- Merge $(Build.SourceBranchName) to $MAINBRANCHNAME --\n   git fetch origin $(Build.SourceBranchName) --prune\n   git merge origin/$(Build.SourceBranchName) -m \"merge $(Build.SourceBranchName) to $MAINBRANCHNAME\" --no-ff --allow-unrelated-histories\n   \n   \n   \n   git push origin $MAINBRANCHNAME\n   git push origin --tags\n   \n  displayName: 'Bash Script'\nIf you are using a classic pipeline add a bash task\n\nAnd add the below script as inline,\ngit config --global user.email \"azuredevops@microsoft.com\"\ngit config --global user.name \"Azure DevOps\"\n\nREPO=\"$(System.TeamFoundationCollectionUri)$(System.TeamProject)/_git/$(Build.Repository.Name)\"\nEXTRAHEADER=\"Authorization: Bearer $(System.AccessToken)\"\ngit -c http.extraheader=\"$EXTRAHEADER\" clone $REPO \ncd $(Build.Repository.Name)\n\nmkdir 'qa1-ause-asy-01'\ncd 'qa1-ause-asy-01'\n\ncp '$(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateForWorkspace.json' .\ncp '$(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateParametersForWorkspace.json' .\n\ncd ..\n\ngit add Template-QA/TemplateForWorkspace.json\ngit add Template-QA/TemplateParametersForWorkspace.json\n\n\nMAINBRANCHNAME=main\ngit config http.$REPO.extraHeader \"$EXTRAHEADER\"\ngit commit -a -m \"added QA json updated files\"\n\necho -- Merge $(Build.SourceBranchName) to $MAINBRANCHNAME --\ngit fetch origin $(Build.SourceBranchName) --prune\ngit merge origin/$(Build.SourceBranchName) -m \"merge $(Build.SourceBranchName) to $MAINBRANCHNAME\" --no-ff --allow-unrelated-histories\n\n\n\ngit push origin $MAINBRANCHNAME\ngit push origin --tags\nyou can update the commit message according to your need,\nIn this script, the following actions are being performed:\n\nThe mkdir command is creating a new directory called QA-env.\nThe cd command is changing the current working directory to QA-env.\nThe cp command is copying the files TemplateForWorkspace.json and TemplateParametersForWorkspace.json from the ARM subdirectory of the default working directory to the current working directory (QA-env).\nThe cd .. command is changing the current working directory back to the parent directory.\nThe git add command is adding the files TemplateForWorkspace.json and TemplateParametersForWorkspace.json to the staging area in Git. This means that these files will be included in the next commit.\n\nonce done you will see the changes getting merged in the repo\n\nThat is it,\nI guess you can try the steps and get back to me if any help is required!\nCredit :\nhttps://learn.microsoft.com/en-us/azure/devops/pipelines/release/?view=azure-devops\nhttps://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/publish-build-artifacts-v1?view=azure-pipelines\nhttps://stackoverflow.com/questions/52837980/how-to-allow-scripts-to-access-oauth-token-from-yaml-builds\nhttps://chuvash.eu/2021/04/09/git-merge-develop-to-main-in-an-azure-devops-release/"
  },
  {
    "objectID": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html#read-my-blogs",
    "href": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html#read-my-blogs",
    "title": "How to publish from Release Pipeline in Azure DevOps",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html#connect-with-me",
    "href": "posts/2022-12-26-publishing-from-azure-devops-release-pipeline.html#connect-with-me",
    "title": "How to publish from Release Pipeline in Azure DevOps",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "",
    "text": "Kunal Das, Author"
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#introduction",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#introduction",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Introduction",
    "text": "Introduction\nAzure Data Factory (ADF) offers a robust platform for data integration and transformation, and when combined with continuous integration and delivery (CI/CD), it becomes a powerhouse. CI/CD in ADF context is the seamless transition of data pipelines across different environments like development, testing, and production. ADF leverages Azure Resource Manager (ARM) templates to encapsulate the configurations of its entities, such as pipelines, datasets, and data flows. There are primarily two recommended ways to transition a data factory across environments:\n\nLeveraging Azure Pipelines for an automated deployment.\nManually uploading an ARM template through the Data Factory user interface integrated with Azure Resource Manager."
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#embracing-source-control-in-adf",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#embracing-source-control-in-adf",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Embracing Source Control in ADF",
    "text": "Embracing Source Control in ADF\nAzure Data Factory‚Äôs integration with ARM templates facilitates the deployment of pipelines. Notably, there‚Äôs a distinct ADF Publish branch and a collaboration branch.\nSteps for Integrating Source Control with Branching Strategy:\n\nInitialize the Git Repository: Start by initializing a single Git repository. This repository will house multiple ADF configurations, each tailored for specific pipelines.\nBranching Blueprint: Designate a unique branch for every ADF, which will act as a collaboration branch. This setup will lead to the creation of distinct folders under the adf_publish branch. It‚Äôs crucial to have separate branches for development to allow individual feature development, testing, and deployment.Important to note that we may not use this branch as we will use automated ARM tempalte publishing methood.\nIntegrate Development Branches with Source Control: Only link the development branches with source control. This ensures continuous validation and checking of the code during its development phase. Keeping UAT/Production deployments separate ensures a clear demarcation between development and deployment phases.\nPipeline Deployment: Utilize the ARM templates produced by ADF to deploy your pipelines, ensuring a uniform deployment process.\nFinal Integration: Post thorough testing, merge the feature branches with the collaboration branch. The final version in the collaboration branch should be the one deployed to production."
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#advantages-of-git-integration-with-azure-data-factory",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#advantages-of-git-integration-with-azure-data-factory",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Advantages of Git Integration with Azure Data Factory",
    "text": "Advantages of Git Integration with Azure Data Factory\n1. Enhanced Source Control: As ADF tasks become increasingly critical, it‚Äôs essential to:\n\nSeamlessly track and audit changes.\nEffortlessly revert unwanted modifications.\n\n2. Flexible Drafting: Unlike direct authoring, which mandates validation for every save, Git integration allows:\n\nDrafting or partial saves.\nIncremental modifications without validation, ensuring only thoroughly tested changes are published.\n\n3. Collaborative Environment & Role-Based Access: Git facilitates:\n\nCollaborative code reviews.\nDifferentiated permissions, dictating who can edit via Git and who has publishing rights.\n\n4. Streamlined CI/CD Process: Git aids in:\n\nAutomating release pipelines upon changes.\nCustomizing ARM template properties for cleaner configuration management.\n\n5. Boosted Performance: ADFs integrated with Git are significantly faster, loading up to a whopping 10 times quicker due to efficient resource downloading.\nIt‚Äôs worth noting that direct authoring in the Azure Data Factory UI becomes disabled once a Git repository is integrated. However, modifications made via PowerShell or SDK are directly published to the Data Factory service, bypassing Git."
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#architecture-flow-diagram",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#architecture-flow-diagram",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Architecture Flow diagram",
    "text": "Architecture Flow diagram"
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#connecting-to-a-git-repository",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#connecting-to-a-git-repository",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Connecting to a Git Repository",
    "text": "Connecting to a Git Repository\nConfiguration using Management Hub:\n\nNavigate to the management hub within the ADF UI.\nUnder the Source control section, select Git configuration.\nIf no repository is linked, click on Configure.\n\nWhen setting up Git in the Azure Portal, certain settings like project name and repository name need to be manually inputted.\n\nAzure Repos Settings:\nThe configuration pane will display various Azure Repos code repository settings. These settings are essential to apply the CI-CD template. For instance:\n\nRepository Type: Specifies the type of the Azure Repos code repository.\nAzure Active Directory: Your Azure AD tenant name.\nAzure Repos Organization: Your Azure Repos organization name.\nProject Name: Your Azure Repos project name.\nRepository Name: Your Azure Repos code repository name.\nCollaboration Branch: Your Azure Repos collaboration branch used for publishing.\nPublish Branch: The branch where ARM templates related to publishing are stored.\nRoot Folder: Your root folder in your Azure Repos collaboration branch.\n\nEnsure you enable the option in the management hub to include global parameters in the ARM template if you have declared any global parameters.\n# Parameter Definitions: Allows configuration without diving deep into the script.\nparameters:\n- name: envTarget\n  displayName: 'Deployment Environment'\n  type: string\n  values:\n  - Stage\n  - Prod\n  \n- name: azureDFName\n  displayName: 'Azure Data Factory Identifier'\n  type: string\n  \n- name: gitADFPath\n  displayName: 'Git Path for ADF Publishing'\n  type: string\n  \n- name: azureRegion\n  displayName: 'Azure Deployment Region'\n  type: string\n  \n- name: azureResourceGroup\n  displayName: 'Resource Group in Azure'\n  type: string\n  \n- name: azureSubID\n  displayName: 'Azure Subscription Identifier'\n  type: string\n  \n- name: azureRMConnectionName\n  displayName: 'Azure Resource Manager Connection Identifier'\n  type: string\n\n- name: sourceDFName\n  displayName: 'Source Data Factory for ARM Template'\n  type: string\n\n- name: targetDFName\n  displayName: 'Target Data Factory for Deployment'\n  type: string\n\n- name: modifyGlobalParams\n  displayName: 'Modify Global Parameters'\n  type: boolean\n  default: false\n\n# Build Phase: Validate and Create ARM templates for Data Factory using npm.\nstages:\n- stage: Construct\n  displayName: 'Compile and Confirm'\n  jobs:\n  - job: CompileAndCheck\n    displayName: 'Compile and Confirm Azure Data Factory'\n    pool:\n      vmImage: 'ubuntu-latest'\n      \n    steps:\n      # Set up Node.js for npm tasks.\n      - task: NodeTool@0\n        inputs:\n          versionSpec: '14.x'\n        displayName: 'Set up Node.js'\n      \n      # Set up required npm packages for Data Factory.\n      - task: Npm@1\n        inputs:\n          command: 'install'\n          verbose: true\n          workingDir: '$(Build.Repository.LocalPath)/data-factory/'\n        displayName: 'Set up npm modules'\n      \n      # Confirm Data Factory setup.\n      - task: Npm@1\n        inputs:\n          command: 'custom'\n          workingDir: '$(Build.Repository.LocalPath)/data-factory/'\n          customCommand: 'run build validate $(Build.Repository.LocalPath)/data-factory /subscriptions/$(azureSubID)/resourceGroups/$(azureResourceGroup)/providers/Microsoft.DataFactory/factories/$(azureDFName)'\n        displayName: 'Confirm Data Factory Setup'\n      \n      # Create ARM template for Data Factory.\n      - task: Npm@1\n        inputs:\n          command: 'custom'\n          workingDir: '$(Build.Repository.LocalPath)/data-factory/'\n          customCommand: 'run build export $(Build.Repository.LocalPath)/data-factory /subscriptions/$(azureSubID)/resourceGroups/$(azureResourceGroup)/providers/Microsoft.DataFactory/factories/$(azureDFName) \"ArmTemplate\"'\n        displayName: 'Create ARM template'\n      \n      # Share the created ARM template for later stages.\n      - task: PublishPipelineArtifact@1\n        inputs:\n          targetPath: '$(Build.Repository.LocalPath)/data-factory/ArmTemplate'\n          artifact: 'ArmTemplateArtifact'\n          publishLocation: 'pipeline'\n        displayName: 'Share ARM template'\n\n# Deployment Phase: Deploy the Data Factory using ARM template.\n- stage: DeployPhase\n  jobs:\n  - deployment: DeployToTarget\n    displayName: 'Deploy to ${{ parameters.envTarget }} | ADF: ${{ parameters.azureDFName }}'\n    dependsOn: Construct\n    condition: succeeded()\n    environment: ${{ parameters.envTarget }}\n    pool:\n      vmImage: 'ubuntu-latest'\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n            # Skip repo checkout for faster deployment.\n            - checkout: none\n            \n            # Retrieve the ARM template from the build phase.\n            - task: DownloadPipelineArtifact@2\n              inputs:\n                buildType: 'current'\n                artifactName: 'ArmTemplateArtifact'\n                targetPath: '$(Pipeline.Workspace)'\n              displayName: \"Retrieve ARM template\"\n\n            # Optionally modify global parameters if needed.\n            - ${{ if eq(parameters.modifyGlobalParams, true) }}:\n              - task: AzurePowerShell@5\n                displayName: '(Optional) Modify Global Parameters'\n                inputs:\n                  azureSubscription: ${{ parameters.azureRMConnectionName }}\n                  azurePowerShellVersion: 'LatestVersion'\n                  ScriptType: 'FilePath'\n                  ScriptPath: '$(Pipeline.Workspace)/GlobalParametersUpdateScript.ps1'\n                  ScriptArguments: '-globalParametersFilePath \"$(Pipeline.Workspace)/*_GlobalParameters.json\" -resourceGroupName \"${{ parameters.azureResourceGroup }}\" -dataFactoryName \"${{ parameters.sourceDFName }}\"'\n\n            # Deactivate ADF Triggers after deployment.\n            - task: toggle-adf-trigger@2\n              inputs:\n                azureSubscription: ${{ parameters.azureRMConnectionName }}\n                ResourceGroupName: ${{ parameters.azureResourceGroup }}\n                DatafactoryName: ${{ parameters.targetDFName }}\n                TriggerStatus: 'stop'\n\n            # Deploy using the ARM template. Override source ADF name with target ADF name.\n            - task: AzureResourceManagerTemplateDeployment@3\n              displayName: 'Deploy using ARM Template'\n              inputs:\n                azureResourceManagerConnection: ${{ parameters.azureRMConnectionName }}\n                subscriptionId: ${{ parameters.azureSubID }}\n                resourceGroupName: ${{ parameters.azureResourceGroup }}\n                location: ${{ parameters.azureRegion }}\n                csmFile: '$(Pipeline.Workspace)/ARMTemplateForFactory.json'\n                csmParametersFile: '$(Pipeline.Workspace)/ARMTemplateParametersForFactory.json'\n                overrideParameters: '-factoryName \"${{ parameters.targetDFName }}\"'\n              \n            # Activate ADF Triggers after deployment.\n            - task: toggle-adf-trigger@2\n              inputs:\n                azureSubscription: ${{ parameters.azureRMConnectionName }}\n                ResourceGroupName: ${{ parameters.azureResourceGroup }}\n                DatafactoryName: ${{ parameters.targetDFName }}\n                TriggerStatus: 'start'"
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#implementing-the-pipeline-template",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#implementing-the-pipeline-template",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Implementing the Pipeline Template",
    "text": "Implementing the Pipeline Template\nThis guide provides a comprehensive walkthrough on setting up and utilizing the Azure Data Factory CI/CD pipeline as defined in the YAML file. The pipeline streamlines the build and deployment of ADF artifacts to designated environments.\nPrerequisites:\n\nAzure Data Factory Instance: An active ADF instance in Azure.\nAzure DevOps: The YAML is tailored for Azure DevOps. Ensure you have an active Azure DevOps organization and project.\nAzure DevOps Agent: The pipeline employs the ubuntu-latest VM image.\nNode.js: The initial stage requires Node.js.\nAzure Subscription: Necessary permissions on your Azure subscription are required.\n\nTo download necessery package for the npm, keep package.json file in the parent directory\n{\n    \"scripts\":{\n        \"build\":\"node node_modules/@microsoft/azure-data-factory-utilities/lib/index\"\n    },\n    \"dependencies\":{\n        \"@microsoft/azure-data-factory-utilities\":\"^1.0.0\"\n    }\n}\nRepository Structure:\nThe ADF code should be housed in a folder named data-factory-directory at the root of your repository.\nSetup & Execution:\n\nAzure Service Connection: Establish a service connection in Azure DevOps linked to your Azure subscription.\nGlobalParametersUpdateScript.ps1: If the modifyGlobalParams flag is set to true, ensure a PowerShell script named GlobalParamsScript.ps1 is present at the root of your repository.\nUsing the Pipeline: Upload the YAML file to your Azure DevOps repository, create a new pipeline, fill in the parameters, trigger the pipeline, and monitor the build and deployment.\n\nRunning the Pipeline\nPush some changes to developmentbranch. and the pipeline will get triggered automatically\n\nOnce the pipeline finished see the artifact that got published.\n\nSimilarly once the pipeline gets approval for deployment, it will deploy the updated template to production,\n\nModifications & Best Practices:\n\nGlobal Parameter Update: Adjust the logic in the optional global parameter update task if needed.\nAdditional Tasks: Insert any extra tasks within the steps section of the stages.\nPermissions: Not every team member should have update permissions. Implement a system where only a select few can publish to the Data Factory.\nUsing Azure Key Vault: For security, store connection strings or passwords in Azure Key Vault or use managed identity authentication for ADF Linked Services.\n\nIn conclusion, integrating CI/CD with Azure Data Factory not only streamlines the deployment process but also enhances collaboration, auditing, and overall efficiency. With the right setup and best practices, teams can ensure seamless and error-free deployments."
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#read-my-blogs",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#read-my-blogs",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2023-11-03-seamless-data-factory-deployment.html#connect-with-me",
    "href": "posts/2023-11-03-seamless-data-factory-deployment.html#connect-with-me",
    "title": "Seamless Integration and Deployment of Azure Data Factory using Azure DevOps",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "üìù Read my articles on Medium\n\n\n\n\n\n\nMarch 2024 - Present\n\nCollaboration with developers to deploy API in microservice based architecture\nBranching strategy, Deployment strategy, approval mechanism, DevOps Workflow design and setup\nFine tuning resources to match the needs of AI Models and Cost optimization\n\n\n\n\nJuly 2022 - March 2024\n\nAccelerated deployment cycle through ETL pipeline automation for a BFS provider\nImplemented CI/CD automation in the Europe region, reducing manual deployment time by half\nCut Data bricks costs by 35% using a microservice architecture on Azure\nDeveloped one-click infrastructure deployment using Azure DevOps and Terraform\n\n\n\n\nNovember 2020 - July 2022\n\nManaged migration of 50+ workloads to ARM-based Azure Cloud\nEstablished serverless notification system with 99% uptime\n\n\n\n\n\nServer Admin at Quess Corp Limited (2019-2020)\nSystem Admin at Precision Infomatic (2017-2019)"
  },
  {
    "objectID": "about.html#professional-journey",
    "href": "about.html#professional-journey",
    "title": "About Me",
    "section": "",
    "text": "March 2024 - Present\n\nCollaboration with developers to deploy API in microservice based architecture\nBranching strategy, Deployment strategy, approval mechanism, DevOps Workflow design and setup\nFine tuning resources to match the needs of AI Models and Cost optimization\n\n\n\n\nJuly 2022 - March 2024\n\nAccelerated deployment cycle through ETL pipeline automation for a BFS provider\nImplemented CI/CD automation in the Europe region, reducing manual deployment time by half\nCut Data bricks costs by 35% using a microservice architecture on Azure\nDeveloped one-click infrastructure deployment using Azure DevOps and Terraform\n\n\n\n\nNovember 2020 - July 2022\n\nManaged migration of 50+ workloads to ARM-based Azure Cloud\nEstablished serverless notification system with 99% uptime\n\n\n\n\n\nServer Admin at Quess Corp Limited (2019-2020)\nSystem Admin at Precision Infomatic (2017-2019)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nMS in Data Science - Liverpool John Moores University, UK\nB. Tech in Electronics and Communication Engineering - Seacom Engineering College, India\nDiploma in Electronics and Telecommunication Engineering - Ramakrishna Mission Shilpamandira, India"
  },
  {
    "objectID": "about.html#certifications",
    "href": "about.html#certifications",
    "title": "About Me",
    "section": "Certifications",
    "text": "Certifications\n\nMicrosoft Azure\n\nAZ-400\nAZ-104\nAZ-204\nAZ-700\nAZ-900\n\n\n\nHashiCorp Terraform\n\nHashiCorp Terraform Associate (003)"
  },
  {
    "objectID": "about.html#technical-expertise",
    "href": "about.html#technical-expertise",
    "title": "About Me",
    "section": "Technical Expertise",
    "text": "Technical Expertise\n\nCore Skills\n\nCloud Platforms: Azure, GCP\nContainerization: Docker, Kubernetes\nCI/CD: Azure DevOps, GitHub Actions, Jenkins\nInfrastructure as Code: Terraform, ARM\nProgramming: Python, Bash, PowerShell\nMonitoring: Grafana\nVersion Control: Git\n\n\nLanguages and Tools:"
  },
  {
    "objectID": "about.html#languages",
    "href": "about.html#languages",
    "title": "About Me",
    "section": "Languages",
    "text": "Languages\n\nBengali - Native\nEnglish - Proficient\nHindi - Proficient\nSpanish - Beginner"
  },
  {
    "objectID": "talks/2024-05-11-containerization-kubernetes.html",
    "href": "talks/2024-05-11-containerization-kubernetes.html",
    "title": "Containerization at Scale: Kubernetes Deep Dive",
    "section": "",
    "text": "Get the Slides here\nüîó Demo Link: You can check out the demo on\nGitHub üì¶\nYoutube ‚ñ∂Ô∏è\nEvent Link\nKubernetes has democratized the deployment and management of containerized applications, making it accessible even at smaller scales with minimal complexity. However, scaling Kubernetes to manage millions of workloads introduces a new set of challenges that go far beyond the capabilities of basic Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA). This presentation explores advanced strategies for scaling Kubernetes efficiently and effectively while maintaining a keen eye on cost optimization."
  },
  {
    "objectID": "talks/2023-12-22-devops-importance.html",
    "href": "talks/2023-12-22-devops-importance.html",
    "title": "Importance of DevOps in Modern Software Development Lifecycle",
    "section": "",
    "text": "Get the Slides here\nCode used in Demo here\nIn the presentation ‚ÄúImportance of DevOps in Modern Software Development Lifecycle‚Äù by Kunal Das, the focus is on the critical role of DevOps in contemporary software development. The presentation elaborates on the definition and core principles of DevOps, highlighting its significance in integrating development and operations for efficient workflow. It also provides an insightful look into DevSecOps tools, underscoring the importance of security in the DevOps process. Furthermore, it covers aspects of Continuous Integration and Deployment, discussing the benefits and challenges of DevOps implementation. The session concludes with a demonstration and a Q&A segment, emphasizing its practical application in the industry."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html",
    "href": "talks/2023-07-11-telecom-churn-prediction.html",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "In the competitive telecom industry, understanding and predicting customer churn is crucial for maintaining profitability. This project focuses on analyzing customer-level data from a leading telecom firm to build predictive models for identifying high-risk churn customers, particularly in the high-value segment. The goal is to provide actionable insights and strategies to reduce customer churn.\n\n\n\nProject Overview\nDataset Description\nChurn Definition\nHigh-Value Churn\nProject Goals\nContents\nUsage\nConclusion\nRepository Link\nAuthors\n\n\n\n\nThe telecom industry faces a significant challenge with customer churn. Acquiring new customers is often more expensive than retaining existing ones. This project aims to analyze customer data to identify key indicators of churn and develop predictive models to target retention efforts effectively.\n\n\n\nThe dataset includes customer-level information focusing on usage, revenue, and other relevant factors in the Indian and Southeast Asian telecom markets.\n\n\n\nChurn is identified as customers who have not used any services (like outgoing calls, internet, SMS, etc.) for a certain period, which is particularly relevant for prepaid customers.\n\n\n\nFocusing on the top 20% of customers, who contribute to approximately 80% of the telecom industry‚Äôs revenue, this project aims to reduce churn in this high-value customer segment.\n\n\n\n\nAnalyze customer behavior and patterns in the telecom dataset.\nIdentify key indicators of churn among high-value customers.\nDevelop predictive models for accurate churn prediction.\nEvaluate model performance and select the most effective one.\nProvide business insights and recommendations based on the analysis.\n\n\n\n\n\ntelecom_churn_data.csv: Dataset with customer-level data.\ntelecom_churn.ipynb: Jupyter Notebook for data preprocessing, feature engineering, model building, and evaluation.\nREADME.md: Overview of the project.\n\n\n\n\n\nDownload telecom_churn_data.csv and place it in the same directory as the Jupyter Notebooks.\nOpen telecom_churn.ipynb and execute the cells for data processing and model evaluation.\nReview the results, insights, and recommendations.\n\n\n\n\nThis project assists telecom companies in predicting and reducing churn among high-value customers, thereby minimizing revenue loss and enhancing customer retention.\n\n\n\nFor a detailed analysis, code implementation, and model evaluation, visit the GitHub Repository.\n\n\n\n\nKunal\nCo-Author: Yogesh Kumar Pati"
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#table-of-contents",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#table-of-contents",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "Project Overview\nDataset Description\nChurn Definition\nHigh-Value Churn\nProject Goals\nContents\nUsage\nConclusion\nRepository Link\nAuthors"
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#project-overview",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#project-overview",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "The telecom industry faces a significant challenge with customer churn. Acquiring new customers is often more expensive than retaining existing ones. This project aims to analyze customer data to identify key indicators of churn and develop predictive models to target retention efforts effectively."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#dataset-description",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#dataset-description",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "The dataset includes customer-level information focusing on usage, revenue, and other relevant factors in the Indian and Southeast Asian telecom markets."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#churn-definition",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#churn-definition",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "Churn is identified as customers who have not used any services (like outgoing calls, internet, SMS, etc.) for a certain period, which is particularly relevant for prepaid customers."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#high-value-churn",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#high-value-churn",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "Focusing on the top 20% of customers, who contribute to approximately 80% of the telecom industry‚Äôs revenue, this project aims to reduce churn in this high-value customer segment."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#project-goals",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#project-goals",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "Analyze customer behavior and patterns in the telecom dataset.\nIdentify key indicators of churn among high-value customers.\nDevelop predictive models for accurate churn prediction.\nEvaluate model performance and select the most effective one.\nProvide business insights and recommendations based on the analysis."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#contents",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#contents",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "telecom_churn_data.csv: Dataset with customer-level data.\ntelecom_churn.ipynb: Jupyter Notebook for data preprocessing, feature engineering, model building, and evaluation.\nREADME.md: Overview of the project."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#usage",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#usage",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "Download telecom_churn_data.csv and place it in the same directory as the Jupyter Notebooks.\nOpen telecom_churn.ipynb and execute the cells for data processing and model evaluation.\nReview the results, insights, and recommendations."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#conclusion",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#conclusion",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "This project assists telecom companies in predicting and reducing churn among high-value customers, thereby minimizing revenue loss and enhancing customer retention."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#repository-link",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#repository-link",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "For a detailed analysis, code implementation, and model evaluation, visit the GitHub Repository."
  },
  {
    "objectID": "talks/2023-07-11-telecom-churn-prediction.html#authors",
    "href": "talks/2023-07-11-telecom-churn-prediction.html#authors",
    "title": "Telecom Churn Prediction",
    "section": "",
    "text": "Kunal\nCo-Author: Yogesh Kumar Pati"
  },
  {
    "objectID": "talks/2023-05-23-lead-scoring-model.html",
    "href": "talks/2023-05-23-lead-scoring-model.html",
    "title": "Lead Scoring Model for Education Institute",
    "section": "",
    "text": "Project Overview\nApproach\n\nData Understanding and Cleaning\nExploratory Data Analysis (EDA)\nFeature Engineering\nModel Building and Evaluation\n\nKey Learnings\nConclusion\nFurther Reading\nCo-Authors\n\n\n\nThis project involves developing a lead scoring model for X Education to identify potential customers with a higher likelihood of conversion. The goal is to achieve a lead conversion rate of around 80%.\n\n\n\nThe approach was divided into four key stages:\n\nData Understanding and Cleaning\nExploratory Data Analysis (EDA)\nFeature Engineering\nModel Building and Evaluation\n\n\n\n\nObjective: Understanding the dataset‚Äôs structure and cleaning the data.\nTasks Performed:\n\nHandling missing values.\nDropping irrelevant columns.\nAddressing data inconsistencies.\n\n\n\n\n\n\nObjective: Gaining insights through visualizations and statistical summaries.\nKey Findings:\n\nImportance of features like total visits, time spent on the website, and page views in predicting conversion.\n\n\n\n\n\n\nObjective: Enhancing the model‚Äôs predictive power through feature transformation and creation.\nTasks Performed:\n\nEncoding categorical variables.\nScaling numerical features.\nHandling outliers.\nSplitting the dataset into training and testing sets.\n\n\n\n\n\n\nModel Used: Logistic Regression.\nPerformance:\n\nAchieved an accuracy of approximately 79.05% on the test set.\nEvaluated using sensitivity, specificity, precision, and the precision-recall curve.\nIdentified areas for improvement in recall.\n\n\n\n\n\n\n\nData Preprocessing: Essential for accurate modeling.\nExploratory Data Analysis (EDA): Crucial for understanding the dataset and feature selection.\nFeature Engineering: Significantly impacts model performance.\nModel Selection and Evaluation: Vital for achieving desired outcomes.\nInterpretability and Explainability: Important for providing actionable insights.\nPrecision-Recall Trade-off: Aids in selecting an optimal threshold for the model.\n\n\n\n\nThe project successfully developed a lead scoring model for X Education, employing a systematic approach and best practices in data preprocessing and modeling."
  },
  {
    "objectID": "talks/2023-05-23-lead-scoring-model.html#project-overview",
    "href": "talks/2023-05-23-lead-scoring-model.html#project-overview",
    "title": "Lead Scoring Model for Education Institute",
    "section": "",
    "text": "This project involves developing a lead scoring model for X Education to identify potential customers with a higher likelihood of conversion. The goal is to achieve a lead conversion rate of around 80%."
  },
  {
    "objectID": "talks/2023-05-23-lead-scoring-model.html#approach",
    "href": "talks/2023-05-23-lead-scoring-model.html#approach",
    "title": "Lead Scoring Model for Education Institute",
    "section": "",
    "text": "The approach was divided into four key stages:\n\nData Understanding and Cleaning\nExploratory Data Analysis (EDA)\nFeature Engineering\nModel Building and Evaluation\n\n\n\n\nObjective: Understanding the dataset‚Äôs structure and cleaning the data.\nTasks Performed:\n\nHandling missing values.\nDropping irrelevant columns.\nAddressing data inconsistencies.\n\n\n\n\n\n\nObjective: Gaining insights through visualizations and statistical summaries.\nKey Findings:\n\nImportance of features like total visits, time spent on the website, and page views in predicting conversion.\n\n\n\n\n\n\nObjective: Enhancing the model‚Äôs predictive power through feature transformation and creation.\nTasks Performed:\n\nEncoding categorical variables.\nScaling numerical features.\nHandling outliers.\nSplitting the dataset into training and testing sets.\n\n\n\n\n\n\nModel Used: Logistic Regression.\nPerformance:\n\nAchieved an accuracy of approximately 79.05% on the test set.\nEvaluated using sensitivity, specificity, precision, and the precision-recall curve.\nIdentified areas for improvement in recall."
  },
  {
    "objectID": "talks/2023-05-23-lead-scoring-model.html#key-learnings",
    "href": "talks/2023-05-23-lead-scoring-model.html#key-learnings",
    "title": "Lead Scoring Model for Education Institute",
    "section": "",
    "text": "Data Preprocessing: Essential for accurate modeling.\nExploratory Data Analysis (EDA): Crucial for understanding the dataset and feature selection.\nFeature Engineering: Significantly impacts model performance.\nModel Selection and Evaluation: Vital for achieving desired outcomes.\nInterpretability and Explainability: Important for providing actionable insights.\nPrecision-Recall Trade-off: Aids in selecting an optimal threshold for the model."
  },
  {
    "objectID": "talks/2023-05-23-lead-scoring-model.html#conclusion",
    "href": "talks/2023-05-23-lead-scoring-model.html#conclusion",
    "title": "Lead Scoring Model for Education Institute",
    "section": "",
    "text": "The project successfully developed a lead scoring model for X Education, employing a systematic approach and best practices in data preprocessing and modeling."
  },
  {
    "objectID": "talks/2024-05-04-containerization-scale.html",
    "href": "talks/2024-05-04-containerization-scale.html",
    "title": "Containerization at Scale: Challenges and Solutions",
    "section": "",
    "text": "Get the Slides here\nLet‚Äôs look at scaling microservices at large scale which may involve many problems, involving cost optimization. This talk will also look at many new technologies which can help us scaling efficiently and beyound the usage of HPA or VPA."
  },
  {
    "objectID": "talks/2024-08-24-policy-control-opa.html",
    "href": "talks/2024-08-24-policy-control-opa.html",
    "title": "Policy Based Control for Cloud Native Environment with OPA",
    "section": "",
    "text": "This presentation focuses on policy-based control for cloud-native environments using Open Policy Agent (OPA) and Gatekeeper, introducing OPA as a general-purpose policy engine and Gatekeeper as its Kubernetes-native implementation."
  },
  {
    "objectID": "talks/2024-08-24-policy-control-opa.html#agenda",
    "href": "talks/2024-08-24-policy-control-opa.html#agenda",
    "title": "Policy Based Control for Cloud Native Environment with OPA",
    "section": "Agenda",
    "text": "Agenda\n\nIntroduction to Policy Enforcement Challenges\n\nTraditional vs.¬†cloud-native environments\n\nOverview of OPA and Gatekeeper\n\nExplanation of Open Policy Agent (OPA)\nIntroduction to Gatekeeper and its relationship with OPA\nCustom Resource Definitions (CRDs) in Gatekeeper\n\nUsage Scenarios\n\nResource constraints\nSecurity enforcement\nCompliance requirements\nCost optimization\nMulti-tenancy\nBest practices enforcement\n\nChallenges\n\nLearning curve\nPerformance impact\nPolicy management\nIntegration complexity"
  },
  {
    "objectID": "talks/2024-08-24-policy-control-opa.html#resources",
    "href": "talks/2024-08-24-policy-control-opa.html#resources",
    "title": "Policy Based Control for Cloud Native Environment with OPA",
    "section": "Resources",
    "text": "Resources\n\nPresentation Slides\nDemo Repository\nEvent Details"
  },
  {
    "objectID": "talks/2024-05-18-introduction-to-keda.html",
    "href": "talks/2024-05-18-introduction-to-keda.html",
    "title": "Introduction to KEDA",
    "section": "",
    "text": "The presentation provided a comprehensive overview of Kubernetes Event-Driven Autoscaling (KEDA). It covered the core concepts and advanced features of KEDA, emphasizing its role in dynamic resource scaling based on event-driven metrics."
  },
  {
    "objectID": "talks/2024-05-18-introduction-to-keda.html#agenda",
    "href": "talks/2024-05-18-introduction-to-keda.html#agenda",
    "title": "Introduction to KEDA",
    "section": "Agenda",
    "text": "Agenda\n\nOverview of KEDA\nKey Features of KEDA\nHow KEDA Works\nUse Cases\nHands-on Demo\nChallenges & Future\nQ&A Session"
  },
  {
    "objectID": "talks/2024-05-18-introduction-to-keda.html#resources",
    "href": "talks/2024-05-18-introduction-to-keda.html#resources",
    "title": "Introduction to KEDA",
    "section": "Resources",
    "text": "Resources\n\nPresentation Slides\nDemo Code\nRecording\nEvent Details"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Tech Journey",
    "section": "",
    "text": "I‚Äôm a passionate DevOps Lead and Cloud Administrator with a drive for creating efficient, scalable technology solutions. My expertise lies in streamlining processes, optimizing infrastructures, and solving complex technical challenges.\n\n\n\nDevOps Leadership: Leading teams in implementing robust CI/CD pipelines and infrastructure automation\nCloud Architecture: Designing and managing solutions across Azure, AWS, and GCP\nCost Optimization: Proven track record of reducing operational costs while improving system performance\nTechnical Innovation: Constantly exploring new technologies and methodologies\n\n\n\n\n\nüìö Pursuing MS in Data Science\nüéì Sharing knowledge through technical writings and talks\nüí° Exploring emerging trends in MLOps and Cloud Native technologies"
  },
  {
    "objectID": "index.html#professional-focus",
    "href": "index.html#professional-focus",
    "title": "Welcome to My Tech Journey",
    "section": "",
    "text": "DevOps Leadership: Leading teams in implementing robust CI/CD pipelines and infrastructure automation\nCloud Architecture: Designing and managing solutions across Azure, AWS, and GCP\nCost Optimization: Proven track record of reducing operational costs while improving system performance\nTechnical Innovation: Constantly exploring new technologies and methodologies"
  },
  {
    "objectID": "index.html#current-endeavors",
    "href": "index.html#current-endeavors",
    "title": "Welcome to My Tech Journey",
    "section": "",
    "text": "üìö Pursuing MS in Data Science\nüéì Sharing knowledge through technical writings and talks\nüí° Exploring emerging trends in MLOps and Cloud Native technologies"
  },
  {
    "objectID": "index.html#recent-articles",
    "href": "index.html#recent-articles",
    "title": "Welcome to My Tech Journey",
    "section": "üìù Recent Articles",
    "text": "üìù Recent Articles\n\n\n\n\n\n\n\nComprehensive Guide: Running GPU Workloads on Kubernetes\n\n\n\n\n\n\n\n\nDec 6, 2024\n\n\n\n\n\n\n\nSetting Up a Comprehensive Python Build Validation Pipeline in Azure DevOps\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\n\n\n\n\n\nExploring Azure Storage Services üåêüóÑÔ∏è\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#highlighted-projects",
    "href": "index.html#highlighted-projects",
    "title": "Welcome to My Tech Journey",
    "section": "üåü Highlighted Projects",
    "text": "üåü Highlighted Projects\n\n\nüí∞ Azure VM Price Checker\nA lightning-fast tool for comparing Azure VM prices across regions. Built with Python and Streamlit, this application leverages the Azure Retail Prices API to provide instant price comparisons across all Azure regions.\nFeatures include:\n\nQuick SKU search with real-time filtering\nComprehensive price comparison across regions\nMultiple pricing models (Spot, Reserved Instances, Dev/Test)\nFast, responsive interface with concurrent API calls\nNo Azure credentials required\n\nTech Stack: Python, Streamlit, Azure Retail Prices API, ThreadPoolExecutor, Pandas"
  },
  {
    "objectID": "index.html#technical-expertise",
    "href": "index.html#technical-expertise",
    "title": "Welcome to My Tech Journey",
    "section": "üí™ Technical Expertise",
    "text": "üí™ Technical Expertise\n\n\nCloud Platforms\n\nAzure\nAWS\nGoogle Cloud\n\n\n\nDevOps Tools\n\nDocker\nKubernetes\nJenkins\nTerraform\n\n\n\nProgramming\n\nPython\nBash\nJava\nC/C++\n\n\n\nOther Skills\n\nMLOps\nData Engineering\nSystem Design\nAutomation"
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html",
    "href": "posts/2023-07-19-mastering-github-private-repos.html",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "",
    "text": "Kunal Das, Author\nIn the modern era of software development, understanding how to effectively use tools like GitHub is crucial. This guide is designed to help you navigate the world of GitHub, specifically focusing on working with private repositories in an enterprise setting. By the end of this guide, you‚Äôll have a solid foundation in GitHub, git, and SSH, empowering you to manage your codebase efficiently and securely."
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#getting-started-with-github-and-git",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#getting-started-with-github-and-git",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "Getting Started with GitHub and Git",
    "text": "Getting Started with GitHub and Git\n\nCreate Your GitHub Account\n\nStart your journey by setting up a GitHub account. GitHub is a web-based hosting service for version control and is a key player in the open-source community. Having a GitHub account opens up a world of opportunities for collaboration and project management.\nhttps://github.com/join\n2. Install Git\nGit is the backbone of GitHub. It‚Äôs a distributed version control system that allows multiple people to work on a project without overwriting each other‚Äôs changes. Download and install git on your local machine to start leveraging its power.\nhttps://git-scm.com/book/en/v2/Getting-Started-Installing-Git\n3. Configure Git\nPersonalize your git setup by adding your username and email. This information will be associated with any commits you make. Open your terminal or shell and type:\ngit config --global user.name \"Your name here\"git config --global user.email \"your_email@example.com\"\nTo enhance your git experience, enable colored output in the terminal and set your preferred editor. This can make navigating git responses easier and ensure you‚Äôre comfortable when git opens an editor for you:\ngit config - global color.ui  \ngit config - global core.editor"
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#securing-your-connection-with-ssh",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#securing-your-connection-with-ssh",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "Securing Your Connection with SSH",
    "text": "Securing Your Connection with SSH\n\nEstablish SSH Connection\n\nSecurity is paramount when working with code, especially in an enterprise setting. SSH, or Secure Shell, is a protocol that provides a secure channel between your local machine and GitHub. You can follow this comprehensive guide for setting up password-less logins. GitHub also provides a detailed guide for generating SSH keys.\nCheck if you have the files ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub. If not, create these public/private keys:\nssh-keygen -t rsa -C \"your_email@example.com\"\nNow to Copy your public key and get ready to paste it into your GitHub account follow the below steps.\n\nUpdate SSH Key in GitHub Account\n\nNavigate to your GitHub Account Settings and click on ‚ÄúSSH Keys‚Äù. Add a new SSH key with a label (like ‚ÄúVs Code‚Äù) and paste the public key you copied earlier.\n\nTo verify your setup, type the following in your terminal:\n\nssh -T git@github.com\nHi username! You‚Äôve successfully authenticated, but GitHub does not provide shell access.\nIf you see the above message, congratulations! You‚Äôre all set to start working with private repositories in an enterprise setting."
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#additional-tips-and-techniques",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#additional-tips-and-techniques",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "Additional Tips and Techniques",
    "text": "Additional Tips and Techniques"
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#leverage-githubs-issue-tracker",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#leverage-githubs-issue-tracker",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "Leverage GitHub‚Äôs Issue Tracker",
    "text": "Leverage GitHub‚Äôs Issue Tracker\nGitHub‚Äôs issue tracker is a powerful tool for managing tasks, bugs, and feature requests. Each issue provides a platform for discussion, allowing team members to communicate about the task at hand."
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#use-branching-strategically",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#use-branching-strategically",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "2. Use Branching Strategically",
    "text": "2. Use Branching Strategically\nBranching is a core feature of git that allows you to work on different features or bugs in isolation. Developing a good branching strategy can help keep your codebase organized and make the development process smoother."
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#take-advantage-of-github-actions",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#take-advantage-of-github-actions",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "3. Take Advantage of GitHub Actions",
    "text": "3. Take Advantage of GitHub Actions\nGitHub Actions is a CI/CD service provided by GitHub. It allows you to automate tasks like building, testing, and deploying your code. This can save you time and help ensure consistent quality in your codebase."
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#protect-sensitive-information-with-.gitignore",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#protect-sensitive-information-with-.gitignore",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "4. Protect Sensitive Information with .gitignore",
    "text": "4. Protect Sensitive Information with .gitignore\nThe .gitignore file allows you to specify files or directories that git should ignore. This is crucial for keeping sensitive information, like API keys or configuration files with passwords, out of your codebase."
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#stay-informed-with-webhooks",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#stay-informed-with-webhooks",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "5. Stay Informed with Webhooks",
    "text": "5. Stay Informed with Webhooks\nWebhooks allow you to set up automatic notifications when specific events occur in your repository. This can help keep you informed about the state of your project and respond quickly to changes.\nBy mastering these tools and techniques, you‚Äôll be well-equipped to manage private repositories in an enterprise setting. Whether you‚Äôre a seasoned developer or just starting out, GitHub offers a wealth of features to streamline your workflow and enhance collaboration."
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#read-my-blogs",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#read-my-blogs",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2023-07-19-mastering-github-private-repos.html#connect-with-me",
    "href": "posts/2023-07-19-mastering-github-private-repos.html#connect-with-me",
    "title": "Mastering Private Repositories in Enterprise with GitHub",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "",
    "text": "Kunal Das, Author\nHere I am going to share some terraform code to deploy ADF, ADLS, ADB, and several other necessary resources."
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#table-of-contents",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#table-of-contents",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nDeploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform\n\nTable of Contents\nResource Group\nAzure Data Factory:\nAzure Data Bricks:\nVirtual network:\nNetwork Security group for ADB:\nPublic subnet for Databricks:\nPrivate subnet for Databricks:\nNetwork security group for Public subnet:\nNetwork security group for Privatesubnet:\nData Lake storage account:\nStorage account container:\nStorage Admin password:\nSQL server :\nSQL Database:\n\n\nLet‚Äôs start with a resource group where we will store all the resources required."
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#resource-group",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#resource-group",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Resource Group",
    "text": "Resource Group\ndata \"azurerm_client_config\" \"Current\" {}\nresource \"azurerm_resource_group\" \"RG\" {\n  name     = var.ResourceGroup.Name\n  location = var.ResourceGroup.Location\n}\npoints to note that we will fetch the RG name and RG location in the next resource declaration."
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#azure-data-factory",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#azure-data-factory",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Azure Data Factory:",
    "text": "Azure Data Factory:\nresource \"azurerm_data_factory\" \"DataFactory\" {\n  name                = \"DataFactory Name\"\n  location            = azurerm_resource_group.RG.location\n  resource_group_name = azurerm_resource_group.RG.name\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#azure-data-bricks",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#azure-data-bricks",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Azure Data Bricks:",
    "text": "Azure Data Bricks:\nresource \"azurerm_databricks_workspace\" \"Databricks\" {\n  location                      = azurerm_resource_group.RG.location\n  name                          = \"Databricks Name\"\n  resource_group_name           = azurerm_resource_group.RG.name\n  managed_resource_group_name   = \"Databricks Managed Resource Group\"\n  sku                           = \"Databricks Sku\"\n\n  custom_parameters {\n    no_public_ip        = true\n    virtual_network_id  = azurerm_virtual_network.DatabricksVnet.id\n    public_subnet_name  = azurerm_subnet.DatabricksSubnetPublic.name\n    private_subnet_name = azurerm_subnet.DatabricksSubnetPrivate.name\n  }\n\n  depends_on = [\n    azurerm_subnet_network_security_group_association.public,\n    azurerm_subnet_network_security_group_association.private\n  ]\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#virtual-network",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#virtual-network",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Virtual network:",
    "text": "Virtual network:\nresource \"azurerm_virtual_network\" \"DatabricksVnet\" {\n  name                     = \"VNET NAME\"\n  resource_group_name      = azurerm_resource_group.RG.name\n  location                 = azurerm_resource_group.RG.location\n  address_space            = [\"VNET CIDR\"]\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#network-security-group-for-adb",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#network-security-group-for-adb",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Network Security group for ADB:",
    "text": "Network Security group for ADB:\nresource \"azurerm_network_security_group\" \"DatabricksNSG\" {\n  name                     = \"VirtualNetwork NSG Name\"\n  resource_group_name      = azurerm_resource_group.RG.name\n  location                 = azurerm_resource_group.RG.location\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#public-subnet-for-databricks",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#public-subnet-for-databricks",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Public subnet for Databricks:",
    "text": "Public subnet for Databricks:\nresource \"azurerm_subnet\" \"DatabricksSubnetPublic\" {\n  name                 = \"VirtualNetwork PublicSubnet Name\"\n  resource_group_name  = azurerm_resource_group.RG.name\n  virtual_network_name = azurerm_virtual_network.DatabricksVnet.name\n  address_prefixes     = [\"VirtualNetwork PublicSubnet CIDR\"]\n  service_endpoints    = [\"Microsoft.Storage\"]\n\n  delegation {\n    name = \"Microsoft.Databricks.workspaces\"\n    service_delegation {\n      name = \"Microsoft.Databricks/workspaces\"\n      actions = [\n        \"Microsoft.Network/virtualNetworks/subnets/join/action\",\n        \"Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\",\n        \"Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\"]\n    }\n  }\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#private-subnet-for-databricks",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#private-subnet-for-databricks",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Private subnet for Databricks:",
    "text": "Private subnet for Databricks:\nresource \"azurerm_subnet\" \"DatabricksSubnetPrivate\" {\n  name                 = \"VirtualNetwork PrivateSubnet Name\"\n  resource_group_name  = azurerm_resource_group.RG.name\n  virtual_network_name = azurerm_virtual_network.DatabricksVnet.name\n  address_prefixes     = [\"VirtualNetwork PrivateSubnet CIDR\"]\n\n  delegation {\n    name = \"Microsoft.Databricks.workspaces\"\n    service_delegation {\n      name = \"Microsoft.Databricks/workspaces\"\n      actions = [\n        \"Microsoft.Network/virtualNetworks/subnets/join/action\",\n        \"Microsoft.Network/virtualNetworks/subnets/prepareNetworkPolicies/action\",\n        \"Microsoft.Network/virtualNetworks/subnets/unprepareNetworkPolicies/action\"]\n    }\n  }\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#network-security-group-for-public-subnet",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#network-security-group-for-public-subnet",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Network security group for Public subnet:",
    "text": "Network security group for Public subnet:\nresource \"azurerm_subnet_network_security_group_association\" \"public\" {\n  subnet_id                 = azurerm_subnet.DatabricksSubnetPublic.id\n  network_security_group_id = azurerm_network_security_group.DatabricksNSG.id\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#network-security-group-for-privatesubnet",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#network-security-group-for-privatesubnet",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Network security group for Privatesubnet:",
    "text": "Network security group for Privatesubnet:\nresource \"azurerm_subnet_network_security_group_association\" \"private\" {\n  subnet_id                 = azurerm_subnet.DatabricksSubnetPrivate.id\n  network_security_group_id = azurerm_network_security_group.DatabricksNSG.id\n}\nNow as all the associated network configuration done let‚Äôs move to the DATA LAKE STORAGE account creation"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#data-lake-storage-account",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#data-lake-storage-account",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Data Lake storage account:",
    "text": "Data Lake storage account:\nresource \"azurerm_storage_account\" \"DataLake\" {\n  name                     = \"DataLake Name\"\n  resource_group_name      = azurerm_resource_group.RG.name\n  location                 = azurerm_resource_group.RG.location\n  account_tier             = \"DataLake Tier\"\n  account_replication_type = \"DataLake Replication\"\n  is_hns_enabled           = true\n  min_tls_version          = \"DataLake TLSVersion\"\n\n  network_rules {\n    # bypass                     = \"AzureServices\"\n    default_action             = \"Allow\"    \n  }\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#storage-account-container",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#storage-account-container",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Storage account container:",
    "text": "Storage account container:\nresource \"azurerm_storage_container\" \"DataLakeContainer\" {  \n  for_each              = \"DataLake Container\"\n  name                  = each.key\n  storage_account_name  = azurerm_storage_account.DataLake.name\n  container_access_type = \"private\"\n}\nNow, let us create SQL related resources"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#storage-admin-password",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#storage-admin-password",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Storage Admin password:",
    "text": "Storage Admin password:\nresource \"random_string\" \"SQLAdminPassword\" {\n  length      = 5\n  special     = true\n  min_upper   = 2\n  min_numeric = 2\n  min_special = 2\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#sql-server",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#sql-server",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "SQL server :",
    "text": "SQL server :\nresource \"azurerm_mssql_server\" \"SQLServer\" {\n  name                         = \"SQLServer Name\"\n  resource_group_name          = azurerm_resource_group.RG.name\n  location                     = azurerm_resource_group.RG.location\n  version                      = \"SQLServer Version\"\n  administrator_login          = \"SQLServer AdministratorLogin\"\n  administrator_login_password = random_string.SQLAdminPassword.result\n  minimum_tls_version          = \"SQLServer  TLS Version\"\n}"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#sql-database",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#sql-database",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "SQL Database:",
    "text": "SQL Database:\nresource \"azurerm_mssql_database\" \"SQLDatabase\" {\n  name           = \"SQLDatabase Name\"\n  server_id      = azurerm_mssql_server.SQLServer.id\n  collation      = \"SQL_collation\"\n  max_size_gb    = \"SQLDatabase MaxSizeGB\"\n  sku_name       = \"SQLDatabase SKU\"\n  zone_redundant = \"SQLDatabase ZoneRedundant\"\n}\nThis is a complete part by part snippets to create a running ADB ADF system, feel free to reach me in case any clarification required! ## Read my blogs:"
  },
  {
    "objectID": "posts/2022-10-12-deploying-azure-services-with-terraform.html#connect-with-me",
    "href": "posts/2022-10-12-deploying-azure-services-with-terraform.html#connect-with-me",
    "title": "Deploying Azure Data Factory, Azure Data bricks, Azure Data Lake storage & MySql DB using Terraform",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "",
    "text": "Kunal Das, Author"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#understanding-gpu-computing-in-kubernetes",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#understanding-gpu-computing-in-kubernetes",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Understanding GPU Computing in Kubernetes",
    "text": "Understanding GPU Computing in Kubernetes\n\nArchitectural Overview\nGPU (Graphics Processing Unit) computing leverages specialized processors originally designed for rendering graphics to perform parallel computations. Unlike CPUs which are optimized for sequential processing with few cores, GPUs contain thousands of smaller cores optimized for handling multiple tasks simultaneously.\n\nKey Components in Kubernetes GPU Architecture:\n\nHardware Layer\n\nPhysical GPU cards (e.g., NVIDIA Tesla, V100, A100)\nPCIe interface connection\nGPU memory (VRAM)\nCUDA cores for parallel processing\n\nDriver Layer\n\nNVIDIA drivers: Interface between hardware and software\nCUDA toolkit: Programming interface for GPU computing\nContainer runtime hooks: Enable GPU access from containers\n\nContainer Runtime Layer\n\nNVIDIA Container Toolkit (nvidia-docker2)\nContainer runtime (containerd/Docker)\nDevice plugin interface\n\nKubernetes Layer\n\nNVIDIA Device Plugin\nKubernetes scheduler\nResource allocation system\n\n\n\n\n\nHow GPU Scheduling Works in Kubernetes\n\nResource Advertisement\n\nThe NVIDIA Device Plugin runs as a DaemonSet\nDiscovers GPUs on each node\nAdvertises GPU resources to Kubernetes API server\nUpdates node capacity with nvidia.com/gpu resource\n\nResource Scheduling Process\nPod Request ‚Üí Scheduler ‚Üí Device Plugin ‚Üí GPU Allocation\n\nPod makes GPU request through resource limits\nKubernetes scheduler finds eligible nodes\nDevice Plugin handles GPU assignment\nContainer runtime configures GPU access\n\nGPU Resource Isolation\n\nEach container gets exclusive access to assigned GPUs\nGPU memory is not oversubscribed\nNVIDIA driver enforces hardware-level isolation\n\n\n\n\nDeep Dive: NVIDIA Device Plugin Workflow\n\nInitialization Phase\nDevice Plugin Start ‚Üí GPU Discovery ‚Üí Resource Registration\n\nScans system for NVIDIA GPUs\nCreates socket for Kubernetes communication\nRegisters as device plugin with kubelet\n\nOperation Phase\nList GPUs ‚Üí Monitor Health ‚Üí Handle Allocation\n\nMaintains list of available GPUs\nMonitors GPU health and status\nHandles allocation requests from kubelet\n\nResource Management\nPod Request ‚Üí Allocation ‚Üí Environment Setup ‚Üí Container Start\n\nMaps GPU devices to containers\nSets up NVIDIA runtime environment\nConfigures container GPU access\n\n\n\n\nMulti-Instance GPU (MIG) Technology\nFor NVIDIA A100 GPUs, MIG allows: 1. Partitioning - Single GPU split into up to 7 instances - Each instance has dedicated: * Compute resources * Memory * Memory bandwidth * Cache\n\nIsolation Levels\n\nHardware-level isolation\nMemory protection\nError containment\nQoS guarantee\n\n\n\n\nGPU Memory Management\n\nAllocation Modes\n\nExclusive process mode\nTime-slicing mode (shared)\nMIG mode (for A100)\n\nMemory Hierarchy\nGlobal Memory ‚Üí L2 Cache ‚Üí L1 Cache ‚Üí CUDA Cores\nResource Limits\n\nGPU memory is not oversubscribed\nContainers see full GPU memory\nMemory limits enforced by NVIDIA driver"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#understanding-the-gpu-workflow-in-kubernetes",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#understanding-the-gpu-workflow-in-kubernetes",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Understanding the GPU Workflow in Kubernetes",
    "text": "Understanding the GPU Workflow in Kubernetes\n\n1. Pod Scheduling Flow\nPod Creation ‚Üí Resource Check ‚Üí Node Selection ‚Üí GPU Binding ‚Üí Container Start\n\nPod Creation\n\nUser creates pod with GPU requirements\nScheduler receives pod specification\n\nResource Check\n\nScheduler checks GPU availability\nValidates node constraints and taints\nConsiders topology requirements\n\nNode Selection\n\nScheduler selects optimal node\nConsiders GPU availability and type\nEvaluates other scheduling constraints\n\nGPU Binding\n\nDevice plugin assigns specific GPUs\nSets up environment variables\nConfigures container runtime\n\nContainer Start\n\nContainer runtime initializes with GPU access\nNVIDIA driver provides GPU interface\nApplication gains GPU access\n\n\n\n\n2. Data Flow in GPU Computing\nApplication ‚Üí CUDA API ‚Üí Driver ‚Üí GPU ‚Üí Memory ‚Üí Results\n\nApplication Layer\n\nMakes CUDA API calls\nManages data transfers\nOrchestrates computations\n\nCUDA Layer\n\nTranslates API calls to driver commands\nManages memory transfers\nHandles kernel execution\n\nDriver Layer\n\nControls GPU hardware\nManages memory allocation\nSchedules operations\n\nHardware Layer\n\nExecutes CUDA kernels\nPerforms memory operations\nReturns results\n\n\n\n\n3. Resource Lifecycle\nAllocation ‚Üí Usage ‚Üí Release ‚Üí Cleanup\n\nResource Allocation\n\nKubernetes reserves GPU\nDevice plugin configures access\nContainer gets exclusive use\n\nResource Usage\n\nApplication uses GPU\nMonitoring tracks utilization\nResource limits enforced\n\nResource Release\n\nPod termination triggers release\nGPU returned to pool\nResources cleaned up\n\nCleanup Process\n\nMemory cleared\nGPU state reset\nResources marked available"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#best-practices-and-guidelines",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#best-practices-and-guidelines",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Best Practices and Guidelines",
    "text": "Best Practices and Guidelines\n\nResource Optimization\n\nBatch Processing\n\nGroup similar workloads\nUse job queues effectively\nImplement proper backoff strategies\n\nMemory Management\n\nMonitor GPU memory usage\nImplement proper cleanup\nUse appropriate batch sizes\n\nCompute Optimization\n\nUse optimal CUDA algorithms\nBalance CPU and GPU work\nMinimize data transfers"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#enough-talk-lets-dig-in",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#enough-talk-lets-dig-in",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Enough talk, let‚Äôs dig in",
    "text": "Enough talk, let‚Äôs dig in"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#prerequisites",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#prerequisites",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting, ensure you have: - An active Kubernetes cluster (e.g., AKS cluster) - kubectl command-line tool installed and configured - Access to create or modify node pools - Helm v3 installed (for NVIDIA device plugin installation)"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#gpu-node-pool-setup",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#gpu-node-pool-setup",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "GPU Node Pool Setup",
    "text": "GPU Node Pool Setup\n\n1. Select appropriate GPU VM size\nChoose a GPU-enabled VM size based on your workload requirements. Common options include: - Standard_NC6s_v3 (1 NVIDIA Tesla V100) - Standard_NC24rs_v3 (4 NVIDIA Tesla V100 with RDMA) - Standard_ND96asr_v4 (8 NVIDIA A100)\n\n\n2. Create GPU node pool\naz aks nodepool add \\\n    --resource-group myResourceGroup \\\n    --cluster-name myAKSCluster \\\n    --name gpunodepool \\\n    --node-count 1 \\\n    --node-vm-size Standard_NC6s_v3 \\\n    --node-taints sku=gpu:NoSchedule \\\n    --enable-cluster-autoscaler \\\n    --min-count 1 \\\n    --max-count 3\nKey parameters explained: - --node-taints: Ensures only GPU workloads are scheduled on these expensive nodes - --enable-cluster-autoscaler: Automatically scales nodes based on demand - --min-count and --max-count: Define scaling boundaries"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#nvidia-driver-installation",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#nvidia-driver-installation",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "NVIDIA Driver Installation",
    "text": "NVIDIA Driver Installation\n\nOption 1: Default AKS Installation\nBy default, AKS automatically installs NVIDIA drivers on GPU-capable nodes. This is the recommended approach for most users.\n\n\nOption 2: Manual Installation (if needed)\nIf you need to manually install or update drivers:\n\nConnect to the node:\n\n# Get node name\nkubectl get nodes\n\n# Connect to node (requires SSH access)\nssh username@node-ip\n\nInstall NVIDIA drivers:\n\n# Update package list\nsudo apt update\n\n# Install ubuntu-drivers utility\nsudo apt install -y ubuntu-drivers-common\n\n# Install recommended NVIDIA drivers\nsudo ubuntu-drivers install\n\n# Reboot node\nsudo reboot"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#nvidia-device-plugin-installation",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#nvidia-device-plugin-installation",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "NVIDIA Device Plugin Installation",
    "text": "NVIDIA Device Plugin Installation\nThe NVIDIA device plugin is required for Kubernetes to recognize and schedule GPU resources.\n\nUsing Helm (Recommended Method)\n\nAdd NVIDIA Helm repository:\n\nhelm repo add nvdp https://nvidia.github.io/k8s-device-plugin\nhelm repo update\n\nInstall the device plugin:\n\nhelm install nvdp nvdp/nvidia-device-plugin \\\n    --version=0.15.0 \\\n    --namespace nvidia-device-plugin \\\n    --create-namespace"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#verification-steps",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#verification-steps",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Verification Steps",
    "text": "Verification Steps\n\nVerify node GPU capability:\n\nkubectl get nodes -o wide\nkubectl describe node &lt;gpu-node-name&gt;\nLook for the following in the output:\nCapacity:\n  nvidia.com/gpu: 1\nAllocatable:\n  nvidia.com/gpu: 1\n\nTest GPU detection:\n\n# Create a test pod\nkubectl run nvidia-smi --rm -it \\\n    --image=nvidia/cuda:11.8.0-base-ubuntu22.04 \\\n    --limits=nvidia.com/gpu=1 \\\n    --command -- nvidia-smi"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#running-gpu-workloads",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#running-gpu-workloads",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Running GPU Workloads",
    "text": "Running GPU Workloads\n\nExample GPU Workload YAML\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  containers:\n  - name: gpu-container\n    image: nvidia/cuda:11.8.0-base-ubuntu22.04\n    command: [\"nvidia-smi\", \"-l\", \"30\"]\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n  tolerations:\n  - key: \"sku\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\nKey components explained: - resources.limits.nvidia.com/gpu: Specifies GPU requirement - tolerations: Matches node taints to allow scheduling - image: Use CUDA-compatible container image\n\n\nDeploy the workload:\nkubectl apply -f gpu-workload.yaml"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#monitoring-and-troubleshooting",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#monitoring-and-troubleshooting",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Monitoring and Troubleshooting",
    "text": "Monitoring and Troubleshooting\n\nMonitor GPU Usage\nUsing Container Insights, you can monitor: - GPU duty cycle - GPU memory usage - Number of GPUs allocated/available\nKey metrics: - containerGpuDutyCycle - containerGpumemoryUsedBytes - nodeGpuAllocatable\n\n\nCommon Troubleshooting Steps\n\nCheck GPU driver status:\n\nkubectl exec -it &lt;pod-name&gt; -- nvidia-smi\n\nVerify NVIDIA device plugin:\n\nkubectl get pods -n nvidia-device-plugin\n\nCheck pod events:\n\nkubectl describe pod &lt;pod-name&gt;"
  },
  {
    "objectID": "posts/2024-12-06-gpu-workloads-kubernetes.html#best-practices-and-advanced-considerations",
    "href": "posts/2024-12-06-gpu-workloads-kubernetes.html#best-practices-and-advanced-considerations",
    "title": "Comprehensive Guide: Running GPU Workloads on Kubernetes",
    "section": "Best Practices and Advanced Considerations",
    "text": "Best Practices and Advanced Considerations\n\n1. Resource Management Strategy\n\nOptimal Resource Allocation\n\nDefine explicit GPU resource requests and limits in pod specifications\nImplement resource quotas at namespace level to control GPU allocation\nUse pod priority classes for critical GPU workloads\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-workload\nspec:\n  priorityClassName: high-priority-gpu\n  containers:\n  - name: gpu-container\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n      requests:\n        nvidia.com/gpu: 1\n\n\nNode Management\n\nImplement node taints and tolerations for GPU nodes\n\n# Node taint example\nkubectl taint nodes gpu-node-1 gpu=true:NoSchedule\n\n# Pod toleration example\ntolerations:\n- key: \"gpu\"\n  operator: \"Equal\"\n  value: \"true\"\n  effect: \"NoSchedule\"\n\nUse node labels for GPU-specific workload targeting\nConfigure node affinity rules for specialized GPU workloads\n\n\n\n\n2. Container Optimization\n\nImage Management\n\nBase image selection:\n\nUse official NVIDIA CUDA images as base\nChoose appropriate CUDA version for your workload\nConsider slim variants for reduced image size\n\n\n\n\nBuild Optimization\n\nImplement multi-stage builds to minimize image size\nInclude only necessary CUDA libraries\nCache commonly used data in persistent volumes\n\n# Example multi-stage build\nFROM nvidia/cuda:11.8.0-base-ubuntu22.04 as builder\n# Build steps...\n\nFROM nvidia/cuda:11.8.0-runtime-ubuntu22.04\nCOPY --from=builder /app/binary /app/\n\n\n\n3. Performance Monitoring and Optimization\n\nMetrics Collection\n\nImplement comprehensive monitoring:\n\nGPU utilization percentage\nMemory usage patterns\nTemperature and power consumption\nError rates and throttling events\n\n\n\n\nAlert Configuration\n# Example PrometheusRule for GPU alerts\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: gpu-alerts\nspec:\n  groups:\n  - name: gpu.rules\n    rules:\n    - alert: HighGPUUsage\n      expr: nvidia_gpu_duty_cycle &gt; 90\n      for: 10m\n\n\n\n4. Cost Management\n\nResource Scheduling\n\nImplement spot instances for fault-tolerant workloads\nUse node auto-scaling based on GPU demand\nConfigure pod disruption budgets for critical workloads\n\n\n\nWorkload Optimization\n# Example CronJob for off-peak processing\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: gpu-batch-job\nspec:\n  schedule: \"0 2 * * *\"  # Run at 2 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: gpu-processor\n            resources:\n              limits:\n                nvidia.com/gpu: 1\n\n\n\n5. Security Implementation\n\nAccess Control\n\nImplement RBAC for GPU resource access\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: gpu-user\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\"]\n  resourceNames: [\"nvidia.com/gpu\"]\n\n\nRuntime Security\n\nEnable SecurityContext for GPU containers\nImplement network policies for GPU workloads\nRegular security scanning of GPU container images\n\n\n\n\n6. Advanced Scenarios\n\nMulti-Instance GPU (MIG) Configuration\n# Example MIG profile configuration\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mig-workload\nspec:\n  containers:\n  - name: gpu-container\n    resources:\n      limits:\n        nvidia.com/mig-1g.5gb: 1\n\n\nRDMA for High-Performance Computing\n\nConfigure RDMA-capable networks\nOptimize for low-latency communication\nImplement proper RDMA security controls\n\n\n\nDevelopment Environment Optimization\n\nSet up GPU sharing for development teams\nImplement resource quotas per development namespace\nCreate development-specific GPU profiles\n\n\n\n\nFinishing Points\n\nRegular Maintenance Checklist\n\nWeekly driver updates verification\nMonthly performance baseline checking\nQuarterly capacity planning review\n\nDocumentation Requirements\n\nMaintain GPU allocation policies\nDocument troubleshooting procedures\nKeep upgrade procedures updated\n\nFuture Considerations\n\nPlan for GPU architecture upgrades\nEvaluate emerging GPU technologies\nMonitor Kubernetes GPU feature development\n\nEmergency Procedures\n\nGPU failure handling protocol\nWorkload failover procedures\nEmergency contact information"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "",
    "text": "Kunal Das, Author"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#design-goals",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#design-goals",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "1. Design Goals",
    "text": "1. Design Goals"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#cost-estimation",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#cost-estimation",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "1.1. Cost estimation",
    "text": "1.1. Cost estimation\nCost estimation is the process of predicting the cost of a project, product, or service. It involves identifying all the resources that will be needed and determining the associated costs to arrive at a total estimate. Cost estimation is a key part of project management and helps stakeholders understand the financial implications of their decisions. It can also help organizations make informed decisions about whether to pursue a project or service and how to allocate budgets for the best return on investment. There are many factors that can affect the cost of a project, including the complexity of the work, the skills and expertise of the team, and the availability of resources."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#options-available",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#options-available",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "1.2. Options available",
    "text": "1.2. Options available\nThere are several options for cost estimation in the cloud:\nCost calculators: Most cloud providers offer cost calculators that allow you to estimate the cost of various cloud resources based on your usage patterns and requirements. These calculators can help you compare the cost of different options and choose the most cost-effective solution.\nCost optimization tools: Many cloud providers offer tools and services to help organizations optimize their resource usage and costs. These tools can help identify cost drivers, forecast future costs, and recommend optimization strategies.\nPricing plans: Many cloud providers offer a variety of pricing plans that offer different levels of resources and support at different price points. Carefully reviewing and comparing these plans can help you choose the one that best meets your needs and budget.\nCost monitoring and management tools: There are a variety of tools and services available that can help you monitor your cloud resource usage and costs in real-time. These tools can alert you to unexpected cost spikes and help you identify opportunities for cost optimization.\nNegotiating with vendors: Some cloud providers may be willing to negotiate pricing or offer discounts for large or long-term commitments. It may be worth negotiating with your provider to see if you can get a better deal on your cloud resources.\nResource optimization: Identifying and eliminating unnecessary or underutilized resources can help reduce your overall cloud costs. This could include retiring or decommissioning old resources, optimizing resource allocation, and consolidating resources where possible."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#available-tools",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#available-tools",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "1.3. Available tools",
    "text": "1.3. Available tools\nThere are several tools available for cost estimation:\nCost calculators: Many cloud providers offer cost calculators that allow you to estimate the cost of various cloud resources based on your usage patterns and requirements. These calculators can help you compare the cost of different options and choose the most cost-effective solution. E.g. Total Cost of Ownership (TCO) Calculator in AWS, The Azure Pricing Calculator, Google Cloud Pricing Calculator\nCost monitoring and management tools: There are a variety of tools and services available that can help you monitor your cloud resource usage and costs in real-time. These tools can alert you to unexpected cost spikes and help you identify opportunities for cost optimization.\nProject management software: Many project management software platforms offer cost estimation and budgeting tools to help organizations plan and track project costs.\nSpecialized cost estimation software: There are also specialized cost estimation software tools that are specifically designed for cost estimation. These tools often offer advanced features and capabilities, such as the ability to create detailed cost breakdowns and incorporate risk analysis.\nInfracost: Infracost is a tool that helps users optimize their costs when using Terraform for infrastructure management. It provides real-time cost estimates for infrastructure resources and suggests ways to reduce costs through optimization.\nTeracost: TeraCost is a cloud cost management and optimization platform that helps organizations optimize their cloud costs by providing visibility into resource usage and costs, identifying opportunities for optimization, and recommending actions to reduce costs. It offers a range of features, including cost forecasting, budget tracking, and resource optimization recommendations, and is designed to work with a variety of cloud providers, including Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP)."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#infracost",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#infracost",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "2. Infracost",
    "text": "2. Infracost"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#overview",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#overview",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "2.1. Overview",
    "text": "2.1. Overview\nInfracost is a tool that is specifically designed to optimize costs in Terraform environments. It works by analysing the resources that are being created in a Terraform configuration and providing recommendations for cost optimization. This can include suggestions for using lower cost resource types, resizing resources to a more cost-effective size, or identifying resources that are no longer needed and can be safely deleted. Infracost also provides real-time cost estimates for the resources in a Terraform configuration, which can help teams make more informed decisions about the cost of their infrastructure. Overall, Infracost is a powerful tool that can help teams save money on their infrastructure costs by identifying opportunities for cost optimization and providing actionable recommendations for implementing those optimizations."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#advantages",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#advantages",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "2.2. Advantages",
    "text": "2.2. Advantages\nInfracost is a tool that can be used for cost optimization in Terraform, a popular Infrastructure as Code (IaC) tool. It provides several advantages over other cost optimization tools available for Terraform:\nReal-time cost estimation: Infracost provides real-time cost estimates for all the resources in your Terraform code, so you can make informed decisions about your infrastructure costs.\nIntegration with Terraform: Infracost integrates seamlessly with Terraform, so you can see the cost estimates for your infrastructure as you write your code. This helps you avoid costly mistakes and optimize your infrastructure costs from the start.\nCustom pricing: Infracost allows you to set custom pricing for your resources, so you can better reflect your organization‚Äôs negotiated rates or your unique usage patterns.\nResource filtering: Infracost allows you to filter resources by tag, type, and name, making it easier to focus on specific resources and optimize their costs.\nResource grouping: Infracost groups similar resources together, making it easier to compare costs and identify opportunities for optimization.\nEasy to use: Infracost is easy to use and requires minimal setup, so you can start optimizing your infrastructure costs quickly and easily.\nOverall, Infracost provides a comprehensive and user-friendly solution for cost optimization in Terraform, making it an excellent choice for organizations looking to optimize their infrastructure costs."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#limitations",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#limitations",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "2.3 Limitations",
    "text": "2.3 Limitations\nInfracost is a useful tool for gaining visibility into the cost of your infrastructure and optimizing your spend on cloud resources. However, it does have some limitations to consider:\nLimited to supported cloud providers: Infracost currently supports only a limited number of cloud providers, including AWS, Azure, and Google Cloud. If you are using a different cloud provider or a combination of multiple providers, Infracost may not be able to provide cost estimates for your infrastructure.\nDependent on accurate resource pricing: Infracost relies on accurate resource pricing information from the supported cloud providers. If the pricing information is incorrect or out of date, Infracost cost estimates may not be accurate.\nLimited to resources supported by the cloud providers: Infracost can only provide cost estimates for resources that are supported by the cloud providers it integrates with. If you are using custom resources or resources from third-party providers, Infracost may not be able to provide cost estimates for them.\nRequires manual integration with infrastructure as code: While Infracost integrates with popular infrastructure as code tools such as Terraform and CloudFormation, it requires manual integration. This means you will need to set up Infracost and configure it to work with your infrastructure as code tools, which can be time-consuming and require some technical expertise.\nLimited to infrastructure costs: Infracost only provides estimates for the cost of your infrastructure resources, such as compute instances and storage. It does not include other costs such as data transfer fees or licensing costs for third-party software.\nLimited to current infrastructure: Infracost only provides cost estimates for your current infrastructure. It does not allow you to compare the cost of different infrastructure configurations or predict the cost of future infrastructure changes.\nOverall, while Infracost is a useful tool for gaining visibility into the cost of your infrastructure and optimizing your spend on cloud resources, it is limited in its scope and may not be suitable for all organizations or infrastructure configurations."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#use-cases",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#use-cases",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "2.4 Use cases",
    "text": "2.4 Use cases\nSome possible use cases for Infracost include:\nIdentifying and reducing over-provisioned resources: Infracost can help you identify resources that are over-provisioned, which means they have more capacity than is necessary to meet your workload demands. By reducing the capacity of these resources, you can save on your cloud costs.\nOptimizing resource usage: Infracost can help you optimize resource usage by identifying underutilized resources and suggesting ways to improve their utilization. For example, if you have a virtual machine that is only running at 20% capacity, Infracost may suggest moving workloads to that machine to better utilize its capacity.\nEstimating costs for infrastructure changes: Infracost can be used to estimate the costs of making changes to your infrastructure, such as adding or removing resources. This can help you plan for changes and ensure that they are cost-effective.\nAnalysing cloud cost trends: Infracost can provide detailed cost breakdowns and graphs that allow you to understand your cloud costs over time and identify trends that may impact your budget. This can help you plan and make decisions to optimize your costs.\nAuditing and compliance: Infracost can be used to audit your infrastructure and ensure that it complies with company policies or regulatory requirements. It can also help you identify resources that may be overpriced or misconfigured, allowing you to take corrective action."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#impact-analysis",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#impact-analysis",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "2.5 Impact analysis",
    "text": "2.5 Impact analysis\nInfracost is a tool that allows users to perform cost analysis on their infrastructure-as-code (IaC) resources. This means that users can see the estimated cost of their resources before they are deployed, allowing them to make informed decisions about their infrastructure and optimize for cost. One key use case for Infracost is in the development phase, where users can use it to identify and address potential cost issues before they become a problem. It can also be used in the operations phase to monitor the ongoing cost of infrastructure and identify opportunities for optimization. By providing visibility into the cost of infrastructure, Infracost can help organizations make informed decisions about how to allocate their resources and stay within budget. Overall, the use of Infracost can have a significant impact on the cost efficiency of an organization‚Äôs infrastructure, helping to reduce waste and ensure that resources are being used effectively."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#workflow",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#workflow",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "3. Workflow",
    "text": "3. Workflow"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#basic-workflow",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#basic-workflow",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "3.1 Basic workflow",
    "text": "3.1 Basic workflow\nThe workflow of infracost involves the following steps:\nInstall the infracost CLI tool: Infracost can be installed as a standalone CLI tool or as a Terraform plugin. To install the CLI tool, you can use the following command:\ncurl -sfL https://raw.githubusercontent.com/infracost/infracost/stable/scripts/install.sh | sh\nInitialize infracost: Once you have installed the infracost CLI tool, you can initialize it by running the following command in your Terraform directory:\ninfracost init\nThis will create a default configuration file (infracost.yml) in your Terraform directory, which you can customize as per your needs.\nAdd infracost to your Terraform workflow: You can add infracost to your Terraform workflow by using the infracost command before or after running terraform plan or terraform apply. For example, you can run the following command to generate a cost estimate before applying your changes:\ninfracost plan\nAlternatively, you can run the following command to generate a cost estimate after applying your changes:\ninfracost apply\nReview the cost estimate: Infracost will generate a cost estimate in the form of a table, which will show you the estimated cost of each resource in your Terraform configuration. You can review the cost estimate and make any necessary changes to your configuration to optimize costs.\nRepeat the process: You can repeat the process of running infracost before or after applying your changes as many times as you like, until you are satisfied with the cost estimate.\nUse infracost with version control: You can also use infracost with version control systems like Git to track changes to your cost estimates over time. This can be useful for comparing the costs of different versions of your Terraform configuration and for identifying cost-saving opportunities.\nUse infracost with CI/CD: You can also integrate infracost with your CI/CD pipeline to automate the process of generating and reviewing cost estimates. This can be especially useful if you have a large number of Terraform configurations that need to be regularly reviewed for cost optimization."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#devops-workflow",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#devops-workflow",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "3.2 DevOps workflow",
    "text": "3.2 DevOps workflow"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#azure-devops",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#azure-devops",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "3.2.1 Azure DevOps",
    "text": "3.2.1 Azure DevOps\nThe workflow for using infracost with Azure DevOps involves integrating infracost into your Azure DevOps pipeline. This can be done by adding the infracost task to your pipeline and configuring it to run at the appropriate stage.\nTo begin, you will need to install the infracost extension from the Azure DevOps marketplace. Next, you will need to create a pipeline and add the infracost task to it. In the task configuration, you will need to specify the path to your Terraform configuration files, as well as any additional arguments that you want to pass to infracost.\nOnce the task is configured, you can run your pipeline as usual. As part of the pipeline execution, infracost will analyze your Terraform configuration and provide cost estimates for the resources that are being created. This information will be displayed in the pipeline output, allowing you to easily see the cost implications of your changes.\nIn addition to running infracost as part of your pipeline, you can also use it to create pull request checks. This allows you to enforce cost limits on pull requests, ensuring that any changes that are made do not exceed a certain budget.\nOverall, integrating infracost into your Azure DevOps workflow can help you better understand the cost implications of your infrastructure changes, and ensure that you are making cost-effective decisions as you develop and deploy your applications."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#jenkins",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#jenkins",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "3.2.2 Jenkins",
    "text": "3.2.2 Jenkins\nThe workflow for using infracost with Jenkins typically involves the following steps:\nInstall the infracost plugin in Jenkins. This can be done through the Jenkins Plugin Manager.\nConfigure the infracost plugin in the Jenkins Global Configuration page. This includes specifying the AWS access keys, the Terraform version, and the desired frequency for cost scans.\nIn your Jenkins job, add a build step to run infracost. This can be done by selecting ‚ÄúInfracost‚Äù from the Add Build Step dropdown menu.\nIn the infracost build step, specify the path to your Terraform configuration files and any additional command line options.\nRun the Jenkins job to trigger the infracost scan. The infracost plugin will scan your Terraform configuration files and generate a report on the estimated cost of your infrastructure.\nView the infracost report in the Jenkins build output. The report will contain a breakdown of the cost estimates for each resource, as well as a total estimate for the entire infrastructure.\nOptionally, you can configure infracost to fail the Jenkins build if the estimated cost exceeds a certain threshold. This can help prevent overspending on infrastructure costs.\nBy integrating infracost into your Jenkins workflow, you can easily track the cost of your infrastructure and make informed decisions about resource allocation and optimization.\nApart from all Infracost supports multiple CI/CD platforms shown below,"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#implementation",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#implementation",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "1. Implementation",
    "text": "1. Implementation"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#steps-for-azure-devops",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#steps-for-azure-devops",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1 Steps for Azure DevOps",
    "text": "4.1 Steps for Azure DevOps"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#installing-extension",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#installing-extension",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1.1 Installing extension",
    "text": "4.1.1 Installing extension\nGo to your organization setting page in Azure Devops\nhttps://dev.azure.com/{org_name}/_settings/extensions?tab=installed\nGo to the marketplace link and install Infracost extension\nhttps://marketplace.visualstudio.com/items?itemName=Infracost.infracost-tasks\nGo to the Infracost Dashboard URL and login with your account\nhttps://dashboard.infracost.io/"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#making-the-repo-ready",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#making-the-repo-ready",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1.2 Making the repo ready",
    "text": "4.1.2 Making the repo ready\nUpon login go to organization settings and copy the API and keep it in safe place for later use\nIf you do not already have a Terraform file, create one or coy from my repo,\nhttps://github.com/Kunaldastiger/Terraform-Sample/blob/main/Virtual-machines/MicrosoftWindowsServer.tf\nApart from this you must have a pipeline yaml file or a pipeline created in classic fashion,\ncode for the same can be found here,\nhttps://github.com/Kunaldastiger/Terraform-Sample/blob/main/azure-deployment.yaml\nNow run the pipeline and check if resource is getting created or not, needless to say, you also need to have a service connection to azure.\nIf you do not have a service connection kindly follow below steps,\nSign in to Azure DevOps and navigate to the Project Settings page.\nIn the left menu, under ‚ÄúPipelines,‚Äù select ‚ÄúService Connections.‚Äù\nClick the ‚ÄúNew Service Connection‚Äù button.\nIn the ‚ÄúAdd new service connection‚Äù dialog, select ‚ÄúAzure Resource Manager‚Äù from the ‚ÄúType‚Äù dropdown menu.\nGive the service connection a name and click ‚ÄúNext.‚Äù\nIn the ‚ÄúAuthorize Azure Resource Manager‚Äù dialog, select the Azure subscription you want to use and click ‚ÄúAuthorize.‚Äù\nAfter the authorization process is complete, click ‚ÄúFinish‚Äù to create the service connection.\nYou can now use this service connection in your Azure DevOps pipelines to access resources in your Azure subscription. For example, you could use it to deploy an application to an Azure web app, create or delete Azure resources, or run Azure Resource Manager templates."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#adding-infracost-code-in-pipeline",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#adding-infracost-code-in-pipeline",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1.3 Adding Infracost code in pipeline",
    "text": "4.1.3 Adding Infracost code in pipeline\nCreate a new pipeline, selecting\n\nAzure Repos Git when prompted in the ‚ÄúConnect‚Äù stage\nSelect the appropriate repo you wish to integrate Infracost with in the ‚ÄúSelect‚Äù stage\nChoose ‚ÄúStarter Pipeline‚Äù in the ‚ÄúConfigure‚Äù stage\nReplace the Starter Pipeline yaml with the following:\n\n# The Azure Pipelines docs (https://docs.microsoft.com/en-us/azure/devops/pipelines/process/tasks) describe other options.\n6. # Running on pull requests to `master` (or your default branch) is a good default.\n7. pr:\n8.   - main\n9. \n10. variables:\n11.   - name: TF_ROOT\n12.     value: Terraform\n13.   # If you use private modules you'll need this env variable to use \n14.   # the same ssh-agent socket value across all steps. \n15.   - name: SSH_AUTH_SOCK\n16.     value: /tmp/ssh_agent.sock\n17.   # This instructs the CLI to send cost estimates to Infracost Cloud. Our SaaS product\n18.   #   complements the open source CLI by giving teams advanced visibility and controls.\n19.   #   The cost estimates are transmitted in JSON format and do not contain any cloud \n20.   #   credentials or secrets (see https://infracost.io/docs/faq/ for more information).\n21.   - name: INFRACOST_ENABLE_CLOUD\n22.     value: true\n23.   # If you're using Terraform Cloud/Enterprise and have variables stored on there\n24.   # you can specify the following to automatically retrieve the variables:\n25.   # env:\n26.   # - name: INFRACOST_TERRAFORM_CLOUD_TOKEN\n27.   #   value: $(tfcToken)\n28.   # - name: INFRACOST_TERRAFORM_CLOUD_HOST\n29.   #   value: app.terraform.io # Change this if you're using Terraform Enterprise\n30. \n31. jobs:\n32.   - job: infracost\n33.     displayName: Run Infracost\n34.     pool:\n35.       vmImage: ubuntu-latest\n36. \n37.     steps:\n38.       # If you use private modules, add a base 64 encoded secret\n39.       # called gitSshKeyBase64 with your private key, so Infracost can access\n40.       # private repositories (similar to how Terraform/Terragrunt does).\n41.       # - bash: |\n42.       #     ssh-agent -a $(SSH_AUTH_SOCK)\n43.       #     mkdir -p ~/.ssh\n44.       #     echo \"$(echo $GIT_SSH_KEY_BASE_64 | base64 -d)\" | tr -d '\\r' | ssh-add -\n45.       #     # Update this to github.com, gitlab.com, bitbucket.org, ssh.dev.azure.com or your source control server's domain\n46.       #     ssh-keyscan ssh.dev.azure.com &gt;&gt; ~/.ssh/known_hosts\n47.       #   displayName: Add GIT_SSH_KEY\n48.       #   env:\n49.       #     GIT_SSH_KEY_BASE_64: $(gitSshKeyBase64)\n50. \n51.       # Install the Infracost CLI, see https://github.com/infracost/infracost-azure-devops#infracostsetup\n52.       # for other inputs such as version, and pricingApiEndpoint (for self-hosted users).\n53.       - task: InfracostSetup@1\n54.         displayName: Setup Infracost\n55.         inputs:\n56.           apiKey: $(infracostApiKey)\n57. \n58.       # Clone the base branch of the pull request (e.g. main/master) into a temp directory.\n59.       # - bash: |\n60.       #     branch=$(System.PullRequest.TargetBranch)\n61.       #     branch=${branch#refs/heads/}\n62.       #     # Try adding the following to git clone if you're having issues cloning a private repo: --config http.extraheader=\"AUTHORIZATION: bearer $(System.AccessToken)\"\n63.       - task: Bash@3\n64.         inputs:\n65.               targetType: 'inline'\n66.               script: 'git clone @dev.azure.com/{orgname}/{project_name}/{repo_path}\"&gt;https://$(PATToken)@dev.azure.com/{orgname}/{project_name}/{repo_path} --branch=main  /tmp/base'\n67.       #   displayName: Checkout base branch\n68. \n69.       # Generate an Infracost cost estimate baseline from the comparison branch, so that Infracost can compare the cost difference.\n70.       - bash: |\n71.           infracost breakdown --path=$(TF_ROOT) \\\n72.                               --format=json \\\n73.                               --out-file=/tmp/infracost-base.json\n74.         displayName: Generate Infracost cost estimate baseline\n75. \n76.       # Generate an Infracost diff and save it to a JSON file.\n77.       - bash: |\n78.           infracost diff --path=$(TF_ROOT) \\\n79.                          --format=json \\\n80.                          --compare-to=/tmp/infracost-base.json \\\n81.                          --out-file=/tmp/infracost.json\n82.         displayName: Generate Infracost diff\n83. \n84.       # Posts a comment to the PR using the 'update' behavior.\n85.       # This creates a single comment and updates it. The \"quietest\" option.\n86.       # The other valid behaviors are:\n87.       #   delete-and-new - Delete previous comments and create a new one.\n88.       #   new - Create a new cost estimate comment on every push.\n89.       # See https://www.infracost.io/docs/features/cli_commands/#comment-on-pull-requests for other options.\n90.       - bash: |\n91.            infracost comment azure-repos --path=/tmp/infracost.json \\\n92.                                          --azure-access-token=$(System.AccessToken) \\\n93.                                          --pull-request=$(System.PullRequest.PullRequestId) \\\n94.                                          --repo-url= '@dev.azure.com/kunaldas0966/test_project/_git/terracost-sample'\"&gt;https://$(PATToken)@dev.azure.com/kunaldas0966/test_project/_git/terracost-sample' \\\n95.                                          --behavior=update\n96.         displayName: Post Infracost comment\nIn the pipeline, variables add the below variables\n\nTo do the same,\nSign into Azure DevOps and navigate to your project.\nIn the left menu, under ‚ÄúPipelines,‚Äù select ‚ÄúPipelines.‚Äù\nSelect the pipeline you want to add a variable to and click the ‚ÄúEdit‚Äù button.\nIn the pipeline editor, click the ‚ÄúVariables‚Äù tab.\nClick the ‚ÄúAdd‚Äù button to add a new variable.\nIn the ‚ÄúAdd variable‚Äù dialog, enter a name and value for the variable. You can also specify whether the variable is secret and should be masked in the log output.\nClick ‚ÄúSave‚Äù to add the variable to the pipeline.\nYou can now use the pipeline variable in your pipeline tasks by using the syntax $(variableName). You can also use pipeline variables to control the flow of your pipeline, by using conditions or branching."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#additional-settings",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#additional-settings",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1.1 Additional settings",
    "text": "4.1.1 Additional settings\nEnable pull request build triggers. Without this, Azure Pipelines do not trigger builds with the pull request ID, thus comments cannot be posted by the integration.\nFrom your Azure DevOps organization, click on your project &gt; Project Settings &gt; Repositories\nSelect the repository that your created the pipeline for in step 1\nSelect the Policies tab and under the Branch Policies select on your default branch (master or main)\nScroll to Build Validation and click + sign to add one if you don‚Äôt have one already\nSet your ‚ÄòBuild pipeline‚Äô to the pipeline you created in step 1, leave ‚ÄòPath filter‚Äô blank, set ‚ÄòTrigger‚Äô to Automatic, and ‚ÄòPolicy requirement‚Äô to Optional (you can also use Required but we don‚Äôt recommend it).\nEnable Azure Pipelines to post pull request comments\nFrom your Azure DevOps organization, click on your project &gt; Project Settings &gt; Repositories &gt; your repository.\nClick on the Securities tab, scroll down to Users and click on the ‚Äò[project name] Build Service ([org name])‚Äô user, and set the ‚ÄòContribute to pull requests‚Äô to Allow.\nIf you are using github instead of azure repo the code for the same is available at below link,\nhttps://marketplace.visualstudio.com/items?itemName=Infracost.infracost-tasks&targetId=10277548-b72b-4200-99f2-565bf446c70d&utm_source=vstsproduct&utm_medium=ExtHubManageList"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#creating-a-pull-request",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#creating-a-pull-request",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1.2 Creating a Pull Request",
    "text": "4.1.2 Creating a Pull Request\nNot you have to create a new branch and make some changes in the new branch to that the resource cost may increase or decrease,\nTo create a new branch and do a pull request in an Azure Repo, you‚Äôll need to follow these steps:\nNavigate to the Azure Repos page in Azure DevOps.\nSelect the repository that you want to create the branch and pull request in.\nOn the repository page, click on the ‚ÄúBranches‚Äù tab.\nClick on the ‚ÄúNew branch‚Äù button.\nEnter a name for the new branch and select the branch that you want to base the new branch on.\nClick on the ‚ÄúCreate branch‚Äù button to create the new branch.\nOnce the new branch has been created, you can switch to it and make the necessary changes. When you‚Äôre ready to submit the changes for review, you can follow the steps above to create a pull request.\nNavigate to the Azure Repos page in Azure DevOps.\nSelect the repository that you want to create the pull request in.\nOn the repository page, click on the ‚ÄúPull requests‚Äù tab.\nClick on the ‚ÄúNew pull request‚Äù button.\nSelect the source and target branches for the pull request. The source branch is the branch that contains the changes that you want to merge, and the target branch is the branch that you want to merge the changes into.\nReview the changes that will be included in the pull request. You can use the ‚ÄúFiles changed‚Äù tab to view the individual changes and make any necessary adjustments.\nAdd a title and optional description for the pull request.\nIf you want to request a review from specific people or teams, you can use the ‚ÄúReviewers‚Äù field to add them.\nWhen you‚Äôre ready to create the pull request, click on the ‚ÄúCreate pull request‚Äù button.\nAfter this you should see your pipeline starts running as the trigger is set as main pull request!"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#viewing-the-dashboard",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#viewing-the-dashboard",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1.3 Viewing the dashboard",
    "text": "4.1.3 Viewing the dashboard\nLog in to https://dashboard.infracost.io/ and select the repo ,\n\nYou should be able to see similar kind of dashboard, there are many things to go through in the GUI,\nThe Infracost dashboard is a web-based tool that provides an overview of the cost of your infrastructure resources. It displays information about the total cost of your resources, as well as detailed information about each resource and its cost.\nThe dashboard is divided into several sections:\nOverview: This section provides a summary of your infrastructure costs, including the total cost of your resources and the breakdown of costs by resource type.\nResources: This section lists all of the infrastructure resources in your project, along with their cost, resource type, and resource ID. You can use this section to view the cost of individual resources and identify opportunities for cost optimization.\nBilling: This section provides information about your billing history, including the total amount billed, the billing period, and the date of the most recent bill.\nCosts by resource type: This section displays a breakdown of your costs by resource type, showing you which types of resources are the most expensive.\nCosts by tag: This section displays a breakdown of your costs by tag, allowing you to see which resources are associated with a particular tag and their costs.\nCost trends: This section displays a chart showing the trend in your infrastructure costs over time. You can use this chart to identify trends in your costs and identify opportunities for cost optimization."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#dashboard-use-cases",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#dashboard-use-cases",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "4.1.1 Dashboard use cases",
    "text": "4.1.1 Dashboard use cases\nThere are several things that you can do from the Infracost dashboard to manage and optimize the cost of your infrastructure resources:\nView a summary of your infrastructure costs: The dashboard provides an overview of your total infrastructure costs, as well as a breakdown of costs by resource type. This can help you understand the overall cost of your infrastructure and identify areas where you might be able to save money.\nView detailed information about individual resources: The dashboard provides detailed information about each infrastructure resource, including its cost, resource type, and resource ID. This can help you understand the cost of individual resources and identify opportunities for cost optimization.\nView your billing history: The dashboard includes a section on billing that provides information about your total amount billed, the billing period, and the date of the most recent bill. This can help you understand your billing history and track changes in your costs over time.\nView costs by resource type and tag: The dashboard provides a breakdown of costs by resource type and tag, which can help you understand which types of resources the most expensive and which resources are are associated with a particular tag.\nView cost trends over time: The dashboard includes a chart showing the trend in your infrastructure costs over time. This can help you identify trends in your costs and identify opportunities for cost optimization."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#navigation",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#navigation",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "1. Navigation",
    "text": "1. Navigation"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#pr-cost-difference",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#pr-cost-difference",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "5.1 PR cost difference",
    "text": "5.1 PR cost difference\nClick on the repo and select the repository you are using in the Infracost dashboard,\n\nHere, you will see all the price difference before you choose to merge the PR."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#detailed-cost-estimation",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#detailed-cost-estimation",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "5.1 Detailed Cost estimation",
    "text": "5.1 Detailed Cost estimation\nClick to see the full cost estimate and it will open the detailed cost estimates in a separate\n\nHere in the diff output tab you can see the difference that will occur if you merge the changes,\n\nClick on the Table output to see all the involved cost,\n\nUnder each resource you can see detailed cost report of the individual items,\n\nYou may also choose to export the data in CSV format by clicking on export data."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#diagram",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#diagram",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "6. Diagram",
    "text": "6. Diagram\nI have prepared a simple flow diagram that is very easy to understand, let‚Äôs go through each block one by one."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#new-branch",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#new-branch",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "6.1 New Branch",
    "text": "6.1 New Branch\nHere basically we are creating a new branch so that, we can start making changes to existing infrastructure, creating a new branch in a version control system like Azure Repos allows you to work on a copy of your codebase and make changes to it without affecting the main branch (often called the ‚Äúmaster‚Äù branch). This is useful when you want to make changes to your infrastructure, as it allows you to test and validate your changes before merging them into the main branch.\nBy creating a new branch, you can make changes to your infrastructure in a safe and isolated environment, without worrying about breaking anything in the main branch. This can help you avoid disruptions to your infrastructure and ensure that any changes you make are well-tested and ready for production."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#making-changes",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#making-changes",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "6.2 Making changes",
    "text": "6.2 Making changes\nWhen you make changes to your codebase in a feature branch, you are working on a copy of the code that is separate from the main branch (often called the ‚Äúmaster‚Äù branch). This allows you to make changes and test them without affecting the main branch, which can be useful when you are developing new features or making changes to your infrastructure. So you can just pull the code to your IDE like VS code and start making changes in the TF file, once done you can then push it to the dev/feature branch,"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#pull-request",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#pull-request",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "6.3 Pull Request",
    "text": "6.3 Pull Request\nCreate a pull request: When you‚Äôre ready to merge your changes into the main branch, you‚Äôll need to create a pull request. This will allow other members of your team to review your changes and decide whether to merge them into the main branch.\nHere once you create a pull request the pipeline will start running,\nOnce pipeline run finishes you can go to Infracost dashboard and view the cost estimation."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#release",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#release",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "6.4 Release",
    "text": "6.4 Release\nIf satisfied with the cost estimation then merge the changes and as soon as it gets merged to the main branch, it will trigger the terraform pipeline which will then create the desired resources."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#scope-for-improvement",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#scope-for-improvement",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "1. Scope for improvement",
    "text": "1. Scope for improvement\nThe pipeline can be further automated to pass the Pull request automatically if certain conditions are met.\nsuppose if cost increases/decreases &lt; 5% then the pipeline should read that and merge the code,\nIt needs further research and collaboration."
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#cost-of-implementation",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#cost-of-implementation",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "2. Cost of Implementation",
    "text": "2. Cost of Implementation\nInfracost is an open-source tool though it has some pricing options, the free version provides plenty of options to get the job done, the features offered in the free versions are:\nOpen source\nGet cost breakdowns and diffs\nCI/CD integrations (GitHub, GitLab, Bitbucket, Azure DevOps‚Ä¶)\nWorks with Terraform Cloud & Terragrunt\nUse our hosted Cloud Pricing API or self-host\nCommunity supported\nFor an annual fee of $30 some additionally functionalities can be availed,\nIn addition to Infracost Community:\nVisibility across all changes, see pull requests that increase/decrease costs the most\nGuardrails with custom messages/notifications/actions\nPolicies in pull requests (e.g.¬†gp2 to gp3)\nCustom price books and discounts\nJira integration\nCustom reports\nGitHub App integration with pull request status/metadata\nTeam management\nSSO"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#read-my-blogs",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#read-my-blogs",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2023-03-07-infrastructure-cost-estimation.html#connect-with-me",
    "href": "posts/2023-03-07-infrastructure-cost-estimation.html#connect-with-me",
    "title": "Cost Estimation for Infrastructure: A Complete Guide",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "",
    "text": "Kunal Das, Author\nKubernetes, often abbreviated as K8s, has revolutionized the world of container orchestration. As its popularity grows, so does the ecosystem around it, offering a myriad of tools designed to simplify, enhance, and optimize the Kubernetes experience. For DevOps engineers, understanding these tools is paramount. In this comprehensive guide, we‚Äôll explore some of the most promising cluster management tools that can elevate your Kubernetes game."
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kustomizer",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kustomizer",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "1. Kustomizer",
    "text": "1. Kustomizer\nKustomizer is not just another configuration management tool. It stands out by allowing Kubernetes native applications to be customized without the need for templates. This means you can manage variations of Kubernetes YAML configurations without diving into complex templating engines.\nLearn more about Kustomizer"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#k9s",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#k9s",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "2. k9s",
    "text": "2. k9s\nImagine having a bird‚Äôs-eye view of your Kubernetes clusters right from your terminal. k9s offers a terminal-based UI that provides real-time insights into cluster activities and resources. It‚Äôs like having a dashboard but with the power of the command line.\nExplore k9s"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kudo",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kudo",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "3. Kudo",
    "text": "3. Kudo\nBuilding Kubernetes Operators can be challenging. Enter KUDO ‚Äî the Kubernetes Universal Declarative Operator. It‚Äôs an open-source toolkit that simplifies the creation of Operators using a declarative approach, making the management of stateful applications on Kubernetes a breeze.\nDiscover Kudo"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#node-problem-detector",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#node-problem-detector",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "4. node-problem-detector",
    "text": "4. node-problem-detector\nEnsuring the health of your nodes is crucial. The node-problem-detector tool identifies common node issues, bridging the gap between the kernel and cluster management layers. It‚Äôs like having a health check-up for your nodes.\nCheck out node-problem-detector"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#k0s",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#k0s",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "5. k0s",
    "text": "5. k0s\nIn environments where resources are limited, such as edge computing or IoT, k0s shines. It‚Äôs a lightweight, certified Kubernetes distribution tailored for such scenarios, ensuring you don‚Äôt compromise on performance.\nDive into k0s"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#helm",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#helm",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "6. Helm",
    "text": "6. Helm\nHelm is often dubbed the ‚Äúpackage manager for Kubernetes.‚Äù It allows users to define, install, and upgrade even the most complex Kubernetes applications using charts, which are packages of pre-configured Kubernetes resources.\nDiscover Helm"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#clusterpedia",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#clusterpedia",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "7. ClusterPedia",
    "text": "7. ClusterPedia\nSearching and managing resources across clusters can be daunting. ClusterPedia, a unified search platform for Kubernetes clusters, streamlines this process, making resource management efficient and hassle-free.\nLearn about ClusterPedia (Note: Link to be updated when available)"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#keda",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#keda",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "8. KEDA",
    "text": "8. KEDA\nEvent-driven architectures are gaining traction. KEDA (Kubernetes-based Event-Driven Autoscaling) is a component that brings event-driven autoscaling to your Kubernetes applications, ensuring they scale based on real-time demand.\nExplore KEDA"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kubectl-snapshot",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kubectl-snapshot",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "9. kubectl snapshot",
    "text": "9. kubectl snapshot\nDocumentation and debugging are made easier with kubectl snapshot. This tool captures the current state of a Kubernetes cluster, providing a snapshot that can be analyzed or shared.\nDiscover kubectl snapshot (Note: Link to be updated when available)"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#cert-manager",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#cert-manager",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "10. Cert-manager",
    "text": "10. Cert-manager\nSecurity is paramount. Cert-manager steps in as a native Kubernetes certificate management controller, automating the issuance and renewal of certificates from various sources, ensuring your applications remain secure.\nCheck out Cert-manager"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#prometheus",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#prometheus",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "11. Prometheus",
    "text": "11. Prometheus\nMonitoring is crucial in a Kubernetes environment. Prometheus, an open-source monitoring and alerting toolkit, integrates seamlessly with Kubernetes, providing insights into your clusters and applications.\nLearn more about Prometheus"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#metalk8s",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#metalk8s",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "12. Metalk8s",
    "text": "12. Metalk8s\nFor those focusing on long-term on-prem deployments, especially on metal machines, Metalk8s is a go-to Kubernetes distribution. It‚Äôs opinionated, ensuring stability and performance in such specific scenarios.\nDive into Metalk8s"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kube-ops-view",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#kube-ops-view",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "13. Kube-ops-view",
    "text": "13. Kube-ops-view\nVisual representation can simplify complex operations. Kube-ops-view offers a read-only system dashboard for multiple K8s clusters, providing a graphical overview of cluster operations.\nExplore Kube-ops-view\nConclusion: The Kubernetes ecosystem is vast and ever-evolving. For DevOps engineers, staying updated with the right tools can make the difference between a smoothly running cluster and a chaotic environment. This guide provides a starting point, but always ensure to evaluate tools based on your unique needs.\nComment down the tools you use I shall be updating the list whenever I find something cool.\nkeep it followed !!"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#read-my-blogs",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#read-my-blogs",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2023-08-25-kubernetes-cluster-management-tools.html#connect-with-me",
    "href": "posts/2023-08-25-kubernetes-cluster-management-tools.html#connect-with-me",
    "title": "Mastering Kubernetes: Essential Cluster Management Tools",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2023-07-07-azure-webapp-cicd-terraform.html",
    "href": "posts/2023-07-07-azure-webapp-cicd-terraform.html",
    "title": "Creating Azure Web App CI/CD with Terraform and Azure DevOps",
    "section": "",
    "text": "Kunal Das, Author\nIntroduction:\nIn today‚Äôs fast-paced development environment, implementing Continuous Integration and Continuous Deployment (CI/CD) is crucial for efficient software delivery. In this tutorial, we will walk through the process of setting up a CI/CD pipeline for an Azure web app using Terraform and Azure DevOps. By following these steps, you can automate the deployment process and ensure high-quality code reaches your production environment.\nPrerequisites:\nBefore we begin, make sure you have the following prerequisites in place:\n- An Azure DevOps account\n- Access to an Azure subscription\n- Basic knowledge of Terraform and Docker\nStep 1: Creating Infrastructure with Terraform\nTo ensure a consistent and repeatable infrastructure setup, we will use Terraform. Terraform allows us to define our infrastructure as code and manage it through version control. Follow these steps to create the necessary infrastructure for your Azure web app:\n1. Set up a new Azure DevOps project.\n2. Create a new repository within your project and clone it to your local machine.\n3. Create a folder structure for your Terraform code, such as ‚Äúinfra/environment1‚Äù and ‚Äúinfra/environment2‚Äù for two environments.\n4. Within each environment folder, create a main.tf file and define the necessary Azure resources, such as resource groups, app services, and databases. Use Terraform modules to ensure a modular and reusable infrastructure setup.\n5. Define input variables in a separate variable.tf file to make your infrastructure code dynamic and easily configurable.\n6. Initialize Terraform within each environment folder by running the `terraform init` command. This will download the required providers and modules.\n7. Create a build pipeline in Azure DevOps to trigger the Terraform deployment. Use the ‚ÄúTerraform CLI‚Äù task to execute Terraform commands, passing the appropriate input variables for each environment.\nStep 2: CI Steps\nIn this step, we will set up Continuous Integration (CI) to ensure code quality and generate artifacts for deployment.\n1. Configure your source code repository in Azure DevOps to trigger the CI pipeline on code changes.\n2. Set up a build pipeline in Azure DevOps with the following steps:\na. Use a task to install the required dependencies for your application, such as Python packages.\nb. Run unit tests using your preferred testing framework (e.g., pytest) to ensure code functionality.\nc.¬†Use linters (flake8, black) and formatters (isort) to enforce code style consistency.\nd.¬†Generate a code coverage report using coverage.py to track the percentage of code covered by tests.\ne. Integrate with SonarQube for static code analysis and maintain code quality.\nf.¬†Build a Docker image containing your application code and dependencies.\ng. Push the Docker image to an Azure Container Registry (ACR) for later use in deployment.\nStep 3: CD ‚Äî Continuous Deployment\nNow that we have our infrastructure in place and a reliable CI process, we can proceed to set up Continuous Deployment (CD) to automate the deployment process.\n1. Determine the deployment strategy based on your requirements. For example:\n‚Äî Two environments (dev and prod) can follow a simple deployment flow where the code is deployed to dev first and then promoted to prod after successful testing.\n‚Äî Three environments (dev, test, and prod) can follow a more complex deployment flow where code moves through each environment sequentially.\n2. Create release pipelines in Azure DevOps for each environment, using the appropriate deployment strategy.\n3. Configure post-deployment tests specific to each environment to ensure the deployed application is functioning as expected.\n4. Add performance testing steps after the deployment to the dev environment to validate system\nperformance under real-world conditions. Use tools like Apache JMeter or Azure Application Insights to conduct performance tests.\nCertainly! Below are the code snippets for each step, following a generic parameterized approach and best practices:\nStep 1: Creating Infrastructure with Terraform\n1. Set up Azure DevOps pipeline:\n‚Äî Create a new pipeline in Azure DevOps and select your repository.\n‚Äî Choose the appropriate trigger for your pipeline, such as a repository branch or pull request.\n2. Configure Terraform backend:\n‚Äî Create an Azure Storage Account to store Terraform state.\n‚Äî Add the following code to your `main.tf` file within each environment folder:\n3. Define variables:\n‚Äî Create a `variables.tf` file within each environment folder to define input variables. Here‚Äôs an example:\n4. Create infrastructure resources:\nAdd the following code to create a resource group and app service plan in the `main.tf` file within each environment folder:\n5. Execute Terraform commands in Azure DevOps pipeline:\nAdd the following steps to your Azure DevOps pipeline YAML file:\nStep 2: CI Steps\n1. Configure CI pipeline triggers and variables:\n‚Äî Define your pipeline triggers and variables in the Azure DevOps YAML file, as per your requirements.\n2. Set up build steps:\n‚Äî Use appropriate task names and commands based on your specific needs. Here‚Äôs an example:\n3. Build and push Docker image:\n‚Äî Use the appropriate task names and commands based on your specific needs. Here‚Äôs an example:\nStep 3: CD ‚Äî Continuous Deployment\n1. Determine deployment strategy and set up release pipelines:\n‚Äî Based on your deployment strategy, define the release pipelines in Azure DevOps with appropriate stages and triggers.\n2. Configure post-deployment tests:\n‚Äî Use the appropriate tasks and scripts to run post-deployment tests. Here‚Äôs an example:\n3. Perform performance testing:\n‚Äî Use the appropriate tasks and scripts to perform performance testing. Here‚Äôs an example using Apache JMeter:\nNote: The provided code snippets serve as examples and may require modifications based on your specific application, environment, and tools being used.\nRemember to update the connection names, resource names, and specific command arguments according to your project setup and requirements."
  },
  {
    "objectID": "posts/2023-07-07-azure-webapp-cicd-terraform.html#read-my-blogs",
    "href": "posts/2023-07-07-azure-webapp-cicd-terraform.html#read-my-blogs",
    "title": "Creating Azure Web App CI/CD with Terraform and Azure DevOps",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2023-07-07-azure-webapp-cicd-terraform.html#connect-with-me",
    "href": "posts/2023-07-07-azure-webapp-cicd-terraform.html#connect-with-me",
    "title": "Creating Azure Web App CI/CD with Terraform and Azure DevOps",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "",
    "text": "Kunal Das, Author\nIt is crucial to have a simplified and effective process for developing, testing, and implementing solutions as data and analytics become more and more important for enterprises. Microsoft‚Äôs cloud-based analytics solution, Synapse Analytics, has strong data warehousing, machine learning, and integration capabilities. However, without the proper equipment and knowledge, building a Continuous Integration and Continuous Deployment (CI/CD) procedure for Synapse can be difficult. Teams can automate the deployment of Synapse solutions thanks to Azure DevOps‚Äô complete collection of CI/CD pipeline capabilities. Using Azure DevOps to streamline Synapse CI/CD, we will examine best practices and implementation advice in this article."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#what-is-synapse-analytics",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#what-is-synapse-analytics",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "What is Synapse Analytics?",
    "text": "What is Synapse Analytics?\nMicrosoft created the cloud-based analytics solution known as Synapse Analytics. It is a platform that gives businesses the ability to ingest, prepare, manage, and provide data for their urgent needs in business intelligence and machine learning. Data integration, big data, data warehousing, and AI are just a few of the tools and services that Synapse Analytics integrates into a single workspace to provide a seamless end-to-end data analytics experience. The platform provides cutting-edge functions like code-free or code-first data integration, big data processing based on SQL and Spark, machine learning, and Power BI reporting capabilities. Synapse Analytics offers a single, end-to-end development and deployment experience while allowing customers to work with their current tools, languages, and frameworks."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#what-is-the-need-for-ci-cd-for-synapse-analytics",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#what-is-the-need-for-ci-cd-for-synapse-analytics",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "what is the need for CI-CD for synapse analytics?",
    "text": "what is the need for CI-CD for synapse analytics?\nFirst of all, Synapse Analytics is a cloud-based analytics service that is frequently updated and added to with new capabilities. The danger of potential bugs and other problems is reduced by a well-defined CI/CD process, which makes sure that the most recent changes are integrated and tested in the current system before being deployed to production.\nSecond, a well-defined CI/CD procedure that guarantees seamless cooperation and effective deployment is crucial because Synapse Analytics frequently involves numerous developers working on various project components.\nLastly, A well-defined CI/CD approach also ensures that the pipeline is tested and verified at every level, resulting in greater quality, more stable solutions, and a quicker time to market. Synapse Analytics frequently entails complicated data processing and integration.\nOverall, teams can manage the development and deployment of solutions more effectively while maintaining high quality and lowering the risk of errors and downtime by adopting a CI/CD process for Synapse Analytics with technologies like Azure DevOps."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#what-are-the-different-components-of-synapse-analytics",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#what-are-the-different-components-of-synapse-analytics",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "What are the different components of Synapse Analytics?",
    "text": "What are the different components of Synapse Analytics?\nA cloud-based analytics service called Synapse Analytics consists of a number of parts that work together to offer a whole analytics solution. Synapse Analytics‚Äô principal parts are:\nSynapse Studio: A web-based integrated development environment (IDE) called Synapse Studio offers a centralized workspace for creating, overseeing, and maintaining Synapse applications. It consists of a number of technologies and services, including big data, machine learning, data warehousing, and data integration.\nSynapse SQL: Users of Synapse SQL can execute SQL queries against both organized and unstructured data using this distributed SQL engine. It supports serverless and provisioned models, enables petabyte-scale data warehousing, and supports both.\nSynapse Spark: Synapse Spark is a big data processing engine that enables customers to use Apache Spark to process massive amounts of data. Big data analytics, offers a high-performance computing environment that supports both batch and real-time data processing.\nSynapse Pipelines: Users can design, orchestrate, and monitor data pipelines using the data integration service known as Synapse Pipelines. It can integrate data from numerous sources and supports both code-free and code-first data integration scenarios.\nSynapse Serverless: Users can analyze data using SQL queries using Synapse Serverless, a serverless SQL pool, without needing to set up or maintain a specific cluster.\nSynapse Dedicated SQL Pool: A dedicated cluster for high-performance data warehousing and analytics is offered via the provided SQL pool known as Synapse.\nSynapse Notebooks: Users can work with code and data in a group setting using Synapse Notebooks, a collaborative notebook environment. Python, Scala, and .NET are just a few of the many programming languages it supports."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#why-it-is-hard-to-implement-ci-cd-for-synapse-analytics",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#why-it-is-hard-to-implement-ci-cd-for-synapse-analytics",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "why it is hard to implement ci cd for synapse analytics?",
    "text": "why it is hard to implement ci cd for synapse analytics?\nFor a number of reasons, implementing a Continuous Integration and Continuous Deployment (CI/CD) procedure for Synapse Analytics can be difficult.\nFirst off, Synapse Analytics is a sophisticated cloud-based analytics service that combines a variety of parts and offerings, each with specific needs and dependencies. It may be challenging to develop a simplified and effective CI/CD pipeline that manages all the many components of the service due to its complexity.\nSecond, it can be difficult to develop a testing and deployment procedure that is quick and successful since Synapse Analytics frequently requires processing and handling massive volumes of data. It can take a lot of time to test and validate data pipelines, and handling enormous amounts of data makes it challenging to set up a testing environment that precisely mimics the production environment.\nThirdly, it can be difficult to make sure that everyone is using the most recent code and data because Synapse Analytics is frequently utilized by numerous developers and teams. Version control problems and other issues may result from this, which may slow down the development and deployment process.\nLast but not least, For Synapse Analytics one needs experience in a variety of fields, including data integration, warehousing, big data, and machine learning, in order to establish a CI/CD process. Teams may find it difficult to locate the necessary skills and materials to build a reliable and effective CI/CD pipeline as a result.\nOverall, For Synapse Analytics one needs to carefully prepare the implementation of a CI/CD process and have a thorough understanding of all the different parts and services involved. To build a streamlined and effective pipeline that can handle the complexity of Synapse Analytics, it‚Äôs crucial to collaborate with professionals in data engineering, DevOps, and cloud computing."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#lets-understand-the-ci--cd-approach",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#lets-understand-the-ci--cd-approach",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "Let‚Äôs understand the CI -CD Approach:",
    "text": "Let‚Äôs understand the CI -CD Approach:\n\n\nResource Groups: Here we have two resource groups, In how enterprise we may have multiple resource groups and multiple synapse workspaces so to deploy that you can use my terraform guide which can be done very easily.\nArtifacts: for artifacts deployment, I have used the ARM template deployment approach and deployed all the artifacts in that way.\n\nThe most important thing is if you have multiple artifacts linked services then you have to manually edit the ARM templates before deployment for it to work.\n3. Dedicated SQL pool: The dedicated SQL pool scripts and stored procedures, views, etc also can be automated using CI-CD but I have used a secondary pipeline for the same, which makes sense as the development of the two will be asynchronous and have a different lifecycle."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#continuous-integration",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#continuous-integration",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "Continuous Integration :",
    "text": "Continuous Integration :\nSo for the example let‚Äôs think that we have two synapse workspace which represents two environments.\n\nNow log into the dev synapse workspace by going to the portal and clicking on the open synapse studio dialog box\n\nOnce you are in dev synapse studio go to Settings and then GIT configuration.\n\nthen click on ‚Äòconfigure‚Äô\n\nselect GITHUB or AZURE DEVOPS in the repository type\n\nThen proceed to select the collaboration branch and publish branch.\n\nThe collaboration branch and publish branch are two crucial ideas in Synapse Studio that are connected to source control and versioning of Synapse artifacts.\nThe collaboration branch where numerous developers can work together on a Single workspace is the collaboration branch. Usually, Synapse artifacts are developed, tested, and validated using this branch. The collaboration branch can be updated by developers after they create and modify Synapse items like pipelines, notebooks, data flows, and SQL scripts. When you establish a new workspace in Synapse Studio, the collaboration branch, which is the default branch, is automatically created.\nThe publish branch however does a different thing, it publishes the ARM template into that branch which contains everything in the artifact be it pipelines, notebooks, and Linked services detail everything!\nOnce you are done it will look something like this,\n\nNow try to commit all and then publish\n\nThis will start generating the ARM templates and save them in the repo mentioned.\nIf you go to the repo you will see a folder showing your synapse workspace name which will contain two ARM templates.\n\nthe build and release concept is kind of this.\n\nSo we are using three branch policy\nDevelop branch: This branch would be the primary branch for developers to work on Synapse artifacts, including pipelines, notebooks, and data flows. Each developer would create their own feature branch from the develop branch, where they would make and commit their changes. When their feature is complete and tested, they would submit a pull request to merge their changes into the develop branch. The development branch would contain the latest tested changes from all the feature branches.\nFeature branch: Each developer would create their own feature branch from the develop branch, where they would make and commit their changes. Feature branches are typically named after the feature or issue that they are addressing. Once a developer has completed their changes and testing, they would submit a pull request to merge their changes into the develop branch.\nMain branch: The main branch would be the publish branch where you would deploy your Synapse artifacts to a live environment. The main branch would only contain the latest, tested changes that have been approved and merged from the develop branch.\nIn this setup, developers would use the develop branch to collaborate and integrate their changes before publishing to the main branch. This enables developers to work in parallel while maintaining a single source of truth for the Synapse artifacts. The use of feature branches ensures that changes are isolated and tested before being integrated into the develop branch. Finally, the main branch would contain only the tested and approved changes that are ready for deployment. By using this branch policy, you can help ensure that changes to your Synapse artifacts are properly tested and validated, and that version control is maintained throughout the development process.\nCreate a pipeline with the below code\n# The pipeline copies files from workspace publish branch and publishes artifact\n# @author  Kunal Das\n# @version 1.0\n# @since   10-11-2022\nname: \n    -synapse-CI-low\ntrigger:\n- workspace_publish\n\npool:\n  vmImage: ubuntu-22.04\n\nsteps:\n\n- task: CopyFiles@2\n  displayName: 'Copy Files to Build Pipeline directory'\n  inputs:\n    SourceFolder: 'synapse-dev'\n    TargetFolder: '$(Build.ArtifactStagingDirectory)/ARM'\n\n- task: PublishPipelineArtifact@1\n  displayName: 'Publish Pipeline Artifact'\n  inputs:\n    targetPath: '$(Build.ArtifactStagingDirectory)'\n    artifact: drop\nThis is a simplified YAML pipeline for Synapse CI/CD that has been named ‚Äòsynapse-CI-low‚Äô. The pipeline triggers whenever a Synapse workspace is published. The pipeline runs on an Ubuntu 22.04 virtual machine.\nThe first step in the pipeline is to copy files from a folder called ‚Äòsynapse-dev‚Äô to a directory called ‚ÄòARM‚Äô in the build pipeline. This step is accomplished using the ‚ÄòCopyFiles‚Äô task.\nThe second and final step in the pipeline is to publish the Synapse artifacts as a pipeline artifacts. This step is accomplished using the ‚ÄòPublishPipelineArtifact‚Äô task. The target path for the artifacts is set to the build artifact staging directory, and the artifact is named ‚Äòdrop‚Äô.\nThis pipeline is a simple example of a Synapse CI/CD pipeline that copies Synapse artifacts from a source directory and publishes them as a pipeline artifact. This pipeline can be further expanded to include additional steps for building, testing, and deploying the Synapse artifacts.\nNow to deploy the ARM template one extension is required, which can be added from the marketplace.\nTo install the extension follow these steps:\n\nLog in to your Azure DevOps organization and navigate to the Extensions tab in the left-hand menu.\nClick on Browse Marketplace to access the Visual Studio Marketplace.\nSearch for the extension you want to install by typing the name of the extension in the search bar.\nClick on the extension from the list of results to open the extension page.\nClick on the Get it free button.\nSelect the Azure DevOps organization where you want to install the extension and review the terms and conditions.\nClick on the Install button to install the extension in your Azure DevOps organization.\n\nNow for release, we need to add the main step:\nwhich is workspace deployment, for that we add the below task\nsteps:\n- task: AzureSynapseWorkspace.synapsecicd-deploy.synapse-deploy.Synapse workspace deployment@2\n  displayName: 'Synpase deployment task for workspace: synapse-qa'\n  inputs:\n    TemplateFile: '$(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateForWorkspace.json'\n    ParametersFile: '$(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateParametersForWorkspace.json'\n    azureSubscription: 'syn-sp'\n    ResourceGroupName: 'synapseQA-RG'\n    TargetWorkspaceName: 'synapse-qa'\n    OverrideArmParameters: '-LS_AKV_QA_properties_typeProperties_baseUrl https://synapse-qa.vault.azure.net/'\n    FailOnMissingOverrides: true\nThis is a YAML code snippet for deploying a Synapse workspace using the Synapse Workspace Deployment task in Azure DevOps. The code contains the following steps:\n\nThe task is defined with the name ‚ÄòSynapse deployment task for workspace: synapse-qa‚Äô and the version number of the Synapse Workspace Deployment task is set to 2.\nThe template and parameter files for the Synapse workspace are specified. The template file is named ‚ÄòTemplateForWorkspace.json‚Äô and the parameters file is named ‚ÄòTemplateParametersForWorkspace.json‚Äô. These files are located in the default working directory of the pipeline.\nThe Azure subscription to be used for the deployment is specified as ‚Äòsyn-sp‚Äô.\nThe resource group name where the Synapse workspace will be deployed is set to ‚ÄòsynapseQA-RG‚Äô.\nThe target Synapse workspace name is set to ‚Äòsynapse-qa‚Äô.\nThe override ARM parameters are specified using the parameter name and the new value. In this case, the parameter name is ‚Äò-LS_AKV_QA_properties_typeProperties_baseUrl‚Äô and the new value is ‚Äòhttps://synapse-qa.vault.azure.net/‚Äô.\nThe ‚ÄòFailOnMissingOverrides‚Äô option is set to true, which means that the deployment will fail if any of the specified override parameters are not found.\n\nIn summary, this code snippet deploys a Synapse workspace to the specified Azure subscription, resource group, and Synapse workspace name. The override ARM parameters allow you to customize the deployment settings for the workspace.\nif you notice when we deploy the ARM template we need to change somethings like\n1 . Sql pool name\n2. Linked Services details\n3. Spark pool name\nIf you have the same name for all these then you may not have to tinker with the ARM template but for others before running the above deployment step we need to modify these.\nsteps:\n- task: PythonScript@0\n  displayName: 'Change SQL Pool'\n  inputs:\n    scriptSource: inline\n    script: |\n     search_text = 'synapsesqlpooldev'\n     replace_text = 'synapsesqlpoolqa'\n     \n     with open(r'$(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateForWorkspace.json', 'r') as file:\n        data = file.read()\n        data = data.replace(search_text, replace_text)\n     with open(r'$(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateForWorkspace.json', 'w') as file:\n        file.write(data)\n     print(\"SQL pool changed\")\n     \nThis is a YAML code snippet for a PythonScript task in Azure DevOps. The task changes the name of a SQL pool in an ARM template file for a Synapse workspace deployment. The code contains the following steps:\n\nThe task is defined with the name ‚ÄòChange SQL Pool‚Äô and the task version is set to 0.\nThe script source is set to ‚Äòinline‚Äô, which means that the script is written in the YAML file directly.\nThe Python script is defined, which replaces the search text ‚Äòsynapsesqlpooldev‚Äô with the replacement text ‚Äòsynapsesqlpoolqa‚Äô in the ARM template file.\nThe ARM template file is read and its contents are stored in the ‚Äòdata‚Äô variable.\nThe ‚Äòreplace‚Äô method is used to replace the search text with the replacement text.\nThe modified data is written back to the ARM template file.\nA message ‚ÄòSQL pool changed‚Äô is printed to the console.\n\nIn summary, this code snippet uses a Python script to modify the ARM template file for a Synapse workspace deployment by changing the name of a SQL pool from ‚Äòsynapsesqlpooldev‚Äô to ‚Äòsynapsesqlpoolqa‚Äô. The modified ARM template file is used in the subsequent tasks for deploying the Synapse workspace.\nThe same kind of scripts can use for Spark pool and linked services as well.\nIf you want to use Powershell here is the command\nLet us change the linked Azure KeyVault for the same.\n(Get-Content -path $(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateForWorkspace.json) -replace 'LS_AKV_DEV','LS_AKV_QA' | Set-Content -Path $(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateForWorkspace.json\n\n\n(Get-Content -path $(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateParametersForWorkspace.json) -replace 'LS_AKV_DEV','LS_AKV_QA' | Set-Content -Path $(System.DefaultWorkingDirectory)/_Synapse-CI-pipeline/drop/ARM/TemplateParametersForWorkspace.json\n\nThe first command replaces the string ‚ÄòLS_AKV_DEV‚Äô with ‚ÄòLS_AKV_QA‚Äô in the contents of the ARM template file named ‚ÄòTemplateForWorkspace.json‚Äô.\nThe second command replaces the same string in the contents of the ARM template parameter file named ‚ÄòTemplateParametersForWorkspace.json‚Äô.\nThe ‚Äò-path‚Äô parameter specifies the file path for each file, which is set to the default working directory in the build pipeline.\nThe ‚Äò-replace‚Äô parameter is used to search for the specified string and replace it with the replacement string in each file.\nThe ‚Äò|‚Äô (pipe) character is used to send the modified contents to the ‚ÄòSet-Content‚Äô command, which writes the modified contents back to the same file.\nThe modified ARM template files are used in the subsequent tasks for deploying the Synapse workspace.\n\nIn summary, this PowerShell script code uses the ‚ÄòGet-Content‚Äô and ‚ÄòSet-Content‚Äô commands to modify the contents of two ARM template files for a Synapse workspace deployment in Azure DevOps. The ‚Äòreplace‚Äô parameter is used to replace a string in each file, and the modified files are used in the subsequent deployment tasks.\nNow if you have some triggers in the synapse workspace it will not deploy as before deployment triggers must be turned off, to achieve this we can add the below task just before deployment\nsteps:\n- task: AzureSynapseWorkspace.synapsecicd-deploy.toggle-trigger.toggle-triggers-dev@2\n  displayName: 'Toggle Azure Synapse Triggers'\n  inputs:\n    azureSubscription: 'syn-sp'\n    ResourceGroupName: 'Synapseqa-RG'\n    WorkspaceName: 'synapse-qa'\n    ToggleOn: false\nThis pipeline step uses the Azure Synapse CI/CD extension to turn off triggers for a Synapse workspace. The task is named ‚ÄúToggle Azure Synapse Triggers‚Äù and the version being used is 2. The task has several inputs including:\n\nazureSubscription: The Azure subscription that contains the Synapse workspace.\nResourceGroupName: The name of the resource group where the Synapse workspace is located.\nWorkspaceName: The name of the Synapse workspace where the triggers will be turned off.\nToggleOn: A boolean value indicating whether to turn on or off the Synapse triggers. In this case, it is set to false, meaning the triggers will be turned off.\n\nOnce you do all these steps you are finally ready to deploy the synapse workspace!!!"
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#lets-look-at-the-high-level-workflow-of-the-things-we-have-done-so-far",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#lets-look-at-the-high-level-workflow-of-the-things-we-have-done-so-far",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "let‚Äôs look at the high-level workflow of the things we have done so far!",
    "text": "let‚Äôs look at the high-level workflow of the things we have done so far!\n\nDevelopers create or modify Synapse artifacts, such as notebooks, and pipelines, in the development environments.\nThe changes are pushed to a source control repository, such as Azure DevOps Repos, GitHub, or Bitbucket.\nA build pipeline is triggered, which compiles the changes, and creates an artifact, such as an ARM template.\nThe artifact is published to a release pipeline, which deploys it to the Synapse workspace.\nThe deployment process may involve creating or updating Synapse artifacts, deploying resources to Azure, and configuring the Synapse workspace.\nAfter the deployment is completed, the triggers for Synapse pipelines, notebooks, and triggers may need to be toggled on or off.\nIf any issues are found during testing, the pipeline may be rolled back or the code may be fixed and re-deployed.\n\nBut this whole CI-CD pipeline will take care of the spark notebooks pipelines and additionally the Linked services and IRs as well but one major thing is the Stored procedures views etc from the dedicated SQL pool, we need some way to publish these changes can be taken care of by using a second Pipeline,"
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#ci-cd-for-dedicated-sql-pool",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#ci-cd-for-dedicated-sql-pool",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "CI-CD for dedicated SQL pool",
    "text": "CI-CD for dedicated SQL pool\nLet‚Äôs look at the high-level workflow\nSure, here‚Äôs a detailed explanation of the CI/CD workflow for a dedicated SQL pool using Azure Data Studio:\n\nConnect to dedicated SQL pool: First, connect to the dedicated SQL pool using Azure Data Studio. This can be done by selecting the dedicated SQL pool in the Object Explorer and providing the necessary login credentials.\nCreate a project: Once connected, create a new project in Azure Data Studio by selecting File -&gt; New Project. Choose the appropriate project type based on the requirements of the dedicated SQL pool.\nAdd source control: Add the project to a source control repository such as Git by selecting View -&gt; Source Control and following the prompts to initialize the repository and commit the project.\nBuild pipeline: Create a build pipeline in Azure DevOps that takes the *.sqlproj file from the source control repository and builds a DACPAC file using the vsbuild Task. This task compiles the SQL scripts and T-SQL code into a single package that can be deployed to the dedicated SQL pool.\nRelease pipeline: Create a release pipeline in Azure DevOps that deploys the DACPAC file to the next environment, such as the QA environment. This can be done using the dacpac Deploy Task. This task deploys the DACPAC file to the specified SQL Server instance or dedicated SQL pool, and can also handle any necessary pre- or post-deployment scripts.\n\nHere is the complete workflow diagram of the same."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#first-lets-setup-the-repository-for-the-sql-pool",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#first-lets-setup-the-repository-for-the-sql-pool",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "First, let‚Äôs setup the repository for the SQL pool",
    "text": "First, let‚Äôs setup the repository for the SQL pool\n\nCreate or use an existing Azure DevOps project.\nClick on Repos from the sidebar.\nClick on New Repository and fill in the necessary details, such as the repo name. In this case, the repo name used in this example is ‚ÄúSynapse_SQLpool‚Äù. Click on Create.\nConfigure Azure Data Studio (ADS) to connect to the above-created repository. Open ADS and select the New Connection option from the welcome page.\n\n\n5. A new pop-up window will come, here put all the details required for the connection.\n\n6. Click on connect and then wait for the browser window to come on screen, here u can normally log in with your azure credentials and after successful login, you will be redirected to the data studio.\n7. At this point if you click on the Connections side pane on the left you will definitely see the connection sub menu there,\n\nNow select the server connection from here and then right-click and select ‚Äôcreate project from database‚Äô option,\n\nKeep the settings as shown in the below image but select the folder and project name appropriately.\n\n\nOnce you are done with it you will see a project from the left side pane\n\n\n\nNow the only step is to add the project to the Azure repo, there are plenty of ways we can do that, but here inside Azure Data studio itself, you can do the same by going into the left pane source control option.\nClick on the three-dot menu and then select the add remote option,\n\n\n\nHere u just have to put the azure repo link, if it fails somehow open the terminal by pressing ‚Äúctrl + `‚Äù and then run the git init command which will initialize the directory,\nOnce u put the azure repo link whatever changes are there in your project can be pushed to the azure repo easily.\nVerify the remote repo by the command ‚Äògit remote -v‚Äô\nThe Sync option can also be used as it will automatically pull and then push the repo from the azure repo to the local system,\n\n\nOpen the azure repo and navigate to your repo and see the files updated,\n\nNow we have to create a build pipeline for that,\ngo to pipeline and a new pipeline,\n\nselect Azure repo and locate your repository.\n\n\nThe third step would be just selecting the starter pipeline option.\n\n\n\nThen in the yaml file paste the below code,\n\n# @Author : Kunal Das\n# Date : 30-12-2022\n\n\nvariables:\n  poolstagingarea: $(Build.ArtifactStagingDirectory)\\poolstaging\n  BuildConfiguration: release\n  SQLPoolartifactname: AzureSQLPool\n  SQLPooldacpacfile: $(System.ArtifactsDirectory)\\$(SQLPoolartifactname)\\synapseSQLpoolDEV.sqlproj\n\n\ntrigger:\n- main\n\n\npool:\n  vmImage: windows-2019\n\n\nstages:\n\n\n- stage: Pooldacpac\n  displayName: 'Build dacpac'\n\n\n  jobs:\n    - job: 'Builddacpac'\n      displayName: 'Build SQL Pool dacpac'\n      \n      steps:\n      - task: VSBuild@1\n        displayName: 'Builds the dacpac'\n        inputs:\n          solution: synapseSQLpoolDEV.sqlproj\n          configuration: $(BuildConfiguration)\n\n\n      - task: PublishBuildArtifacts@1\n        displayName: 'Publishes dacpac as an artifact'\n        # Publishes the dacpac as part of an artifact within Azure DevOps\n        inputs:\n          PathtoPublish: 'bin\\$(BuildConfiguration)'\n          ArtifactName: $(SQLPoolartifactname)\n          publishLocation: 'Container'\n\nAnd Done! The build pipeline is ready, you may notice I have given the main branch as a trigger at the top so every time someone updates the main branch it triggers the main pipeline.\nSave and run the pipeline, you can see all jobs running perfectly!!!\n\n\n\nClick on the pipeline and see the job artifact in this case the dacpac file has been published."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#implementing-the-release-pipeline-to-deploy-the-above-generated-build-artifacts",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#implementing-the-release-pipeline-to-deploy-the-above-generated-build-artifacts",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "IMPLEMENTING THE RELEASE PIPELINE TO DEPLOY THE ABOVE-GENERATED BUILD ARTIFACTS",
    "text": "IMPLEMENTING THE RELEASE PIPELINE TO DEPLOY THE ABOVE-GENERATED BUILD ARTIFACTS\n\nNow the release pipeline can be created easily by going to pipeline -&gt; release and then selecting create new release pipeline.\n\n\n\nGo to the pipeline and add just one task ‚ÄúSQL dacpac deployment‚Äù\n\n\n\nThis task just points to the QA or the next environment and then just takes the dacpac file to deploy it in the next environments.\n\nThen locate and select the dacpac file.\n\n\nPass an additional argument in the field Additional SqlPackage.exe Arguments as /p: BlockOnPossibleDataLoss=false\nJust save and run the pipeline.\nThe job will run and deploy the changes to the next environment,\n\n\nIn conclusion, the effectiveness and collaboration of data engineering and BI projects can be significantly enhanced by connecting Azure DevOps with Azure Synapse Analytics workspace. You may connect your Azure DevOps project with your Synapse workspace by following the instructions in this article, which will also help you optimize your CI/CD pipeline for better data asset distribution and maintenance. To ensure a seamless and successful implementation, it is crucial to test carefully and make any necessary adjustments. Your firm can benefit from a smooth DevOps and data analytics process with proper design and implementation."
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#read-my-blogs",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#read-my-blogs",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "Read my blogs:",
    "text": "Read my blogs:"
  },
  {
    "objectID": "posts/2023-02-23-streamlining-synapse-cicd.html#connect-with-me",
    "href": "posts/2023-02-23-streamlining-synapse-cicd.html#connect-with-me",
    "title": "Streamlining Synapse CI/CD & Dedicated SQL pool with Azure DevOps",
    "section": "Connect with Me:",
    "text": "Connect with Me:"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Whether you have a question, a project idea, or just want to say hi, I‚Äôd love to hear from you!\n\n\n\n\nWant personalized guidance on DevOps, Cloud Technologies, or Career Development? Visit my Topmate Profile to book a one-on-one session.\n\n  \n    \n    15-minute focused discussion\n    Schedule Now\n  \n  \n  \n    \n    30-minute detailed consultation\n    Schedule Now\n  \n\n  \n    \n    Book on Topmate for technical guidance\n    Book Session\n  \n\n\n\n\n\n\n\n\nEmail: contactkunalmail.region943@passinbox.com\nLinkedIn: kunaldaskd\nTwitter: @kunald_official\nPortfolio: My Website\n\n\n\n\nBased in Bengaluru, India\n\n  \n    \n    \n  \n\n\n\n\n\n\nLooking for consulting in: - DevOps Implementation - Cloud Architecture - Infrastructure Automation - Technical Mentorship\nBook a session through Topmate or Calendly to discuss your needs."
  },
  {
    "objectID": "contact.html#book-a-consultation",
    "href": "contact.html#book-a-consultation",
    "title": "Contact",
    "section": "",
    "text": "Want personalized guidance on DevOps, Cloud Technologies, or Career Development? Visit my Topmate Profile to book a one-on-one session.\n\n  \n    \n    15-minute focused discussion\n    Schedule Now\n  \n  \n  \n    \n    30-minute detailed consultation\n    Schedule Now\n  \n\n  \n    \n    Book on Topmate for technical guidance\n    Book Session"
  },
  {
    "objectID": "contact.html#direct-contact",
    "href": "contact.html#direct-contact",
    "title": "Contact",
    "section": "",
    "text": "Email: contactkunalmail.region943@passinbox.com\nLinkedIn: kunaldaskd\nTwitter: @kunald_official\nPortfolio: My Website"
  },
  {
    "objectID": "contact.html#location",
    "href": "contact.html#location",
    "title": "Contact",
    "section": "",
    "text": "Based in Bengaluru, India"
  },
  {
    "objectID": "contact.html#professional-services",
    "href": "contact.html#professional-services",
    "title": "Contact",
    "section": "",
    "text": "Looking for consulting in: - DevOps Implementation - Cloud Architecture - Infrastructure Automation - Technical Mentorship\nBook a session through Topmate or Calendly to discuss your needs."
  }
]